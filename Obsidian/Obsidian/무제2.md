

오케이, 전체 프로젝트 코드를 구조에 맞춰 작성해 보겠습니다. 상당히 긴 코드가 될 것이며, 실제 운영 환경에서는 충분한 테스트와 검증이 필요합니다. 특히 API 키 관리, 오류 처리, 리스크 관리에 주의해야 합니다.

**참고:**

*   이 코드는 완전한 프로덕션 레벨 코드가 아니며, 시작점으로 사용해야 합니다.
*   에러 핸들링, 로깅, 재시도 로직 등은 기본적인 수준으로 포함되어 있으며, 실제 환경에서는 더 강화해야 합니다.
*   `python-binance` 라이브러리는 비동기 기능을 사용하기 위해 특정 버전 이상이 필요할 수 있습니다. (`pip install python-binance`)
*   `mariadb`, `PyYAML`, `fastapi`, `uvicorn`, `pandas`, `numpy`, `websockets` 라이브러리가 필요합니다. (`pip install mariadb PyYAML fastapi "uvicorn[standard]" pandas numpy websockets`)
*   Dollar Volume Delta 등 일부 지표는 Aggregate Trade Stream을 실시간으로 처리해야 정확하게 계산할 수 있습니다. 여기서는 Taker Buy Volume 비율을 사용하는 것으로 간소화합니다. 정확한 구현을 위해서는 `data_handler`와 `binance_interface` 수정이 필요합니다.
*   실제 거래 전에는 반드시 **페이퍼 트레이딩** 또는 **테스트넷** 모드에서 장기간 테스트하세요.

---

**1. `config/config.yaml`** (이전 답변과 동일)

```yaml
# General Settings
app_name: "MultiFactorCryptoBotV2"
trading_mode: "paper" # Options: paper, testnet, live
log_level: "INFO" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
update_interval_seconds: 60 # How often to fetch data and calculate signals (e.g., 60 for 1m candles)
data_fetch_limit: 500 # Number of historical candles to fetch initially
kline_interval: "1m" # Binance Kline interval

# Binance API Credentials (Required for live/testnet modes)
# USE ENVIRONMENT VARIABLES OR SECRETS MANAGER IN PRODUCTION
binance:
  api_key: ""
  api_secret: ""
  testnet_api_key: ""
  testnet_api_secret: ""

# MariaDB Database Connection
database:
  host: "127.0.0.1" # Use 127.0.0.1 instead of localhost if connection issues arise
  port: 3306
  user: "crypto_bot_user"
  password: "your_db_password"
  database: "crypto_trading_v2"

# Trading Universe
universe:
  type: "list" # options: all, list
  symbol_list:
    - "BTCUSDT"
    - "ETHUSDT"
    # - "BNBUSDT"
  # exclude_symbols:
  #   - "DOGEUSDT"

# Trading Parameters
leverage: 5 # Default leverage for symbols
initial_capital_usd: 10000 # Starting capital for paper/testnet simulation
max_position_risk_per_trade: 0.01 # Max % of capital to risk on a single position entry (Needs careful implementation in position sizing)
max_total_risk_exposure: 0.10 # Max % of capital at risk across all open positions (Needs careful implementation)
target_signal_volatility: 0.2 # Target volatility for the final scaled factor

# Web UI Settings
web_ui:
  host: "127.0.0.1"
  port: 8000

# Strategy Specific Parameters (Keep defaults from description)
strategy_v2:
  lookback_long: 240 # General long lookback
  lookback_medium: 120
  lookback_short: 24
  funding_roll: 144
  mr_roll: 96
  flow_roll: 120
  perf_roll: 192
  perf_smooth_span: 60
```

---

**2. `database/sql/create_tables.sql`** (이전 답변과 유사, 일부 수정)

```sql
-- Trade history
CREATE TABLE IF NOT EXISTS trades (
    id INT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    symbol VARCHAR(20) NOT NULL,
    mode VARCHAR(10) NOT NULL, -- paper, testnet, live
    order_id VARCHAR(50) NULL, -- Can be null for paper trades
    client_order_id VARCHAR(50) NULL,
    side VARCHAR(4) NOT NULL, -- BUY, SELL
    position_side VARCHAR(5) NOT NULL DEFAULT 'BOTH', -- LONG, SHORT, BOTH (Binance uses BOTH for one-way mode)
    order_type VARCHAR(10) NOT NULL DEFAULT 'MARKET',
    price DECIMAL(20, 8) NOT NULL,
    quantity DECIMAL(20, 8) NOT NULL,
    quote_quantity DECIMAL(20, 8) NOT NULL, -- price * quantity
    commission DECIMAL(20, 10) NULL DEFAULT 0.0,
    commission_asset VARCHAR(10) NULL,
    realized_pnl DECIMAL(20, 8) NULL DEFAULT 0.0, -- PNL realized on closing this trade part
    is_maker BOOLEAN NULL,
    trade_time DATETIME(3) NOT NULL, -- Actual execution time from exchange/simulation
    strategy_factor DECIMAL(18, 9) NULL, -- Factor value at the time of trade decision
    INDEX idx_symbol_time (symbol, timestamp),
    INDEX idx_mode_time (mode, timestamp)
);

-- Current Positions (snapshot)
CREATE TABLE IF NOT EXISTS positions (
    id INT AUTO_INCREMENT PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    mode VARCHAR(10) NOT NULL,
    quantity DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- Positive for long, negative for short
    entry_price DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- Average entry price
    mark_price DECIMAL(20, 8) NULL, -- Last known mark price
    unrealized_pnl DECIMAL(20, 8) NULL DEFAULT 0.0,
    leverage INT NOT NULL,
    initial_margin DECIMAL(20, 8) NULL DEFAULT 0.0, -- Approximate initial margin for the position
    last_update_ts DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
    UNIQUE KEY unique_symbol_mode (symbol, mode) -- Ensures one row per symbol per mode
);

-- Performance Snapshots (e.g., daily or hourly)
CREATE TABLE IF NOT EXISTS performance (
    id INT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME NOT NULL,
    mode VARCHAR(10) NOT NULL,
    total_equity DECIMAL(20, 8) NOT NULL, -- Starting capital + realized PnL + unrealized PnL - fees
    realized_pnl_cumulative DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- Cumulative realized PnL since start
    unrealized_pnl DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- Current total unrealized PnL
    total_pnl_cumulative DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- realized + unrealized
    total_fees_cumulative DECIMAL(20, 8) NOT NULL DEFAULT 0.0, -- Cumulative fees paid
    trade_count_cumulative INT NOT NULL DEFAULT 0, -- Cumulative number of trades
    UNIQUE KEY unique_mode_time (mode, timestamp)
);

-- Signals (Optional: for debugging/analysis)
CREATE TABLE IF NOT EXISTS signals (
    id INT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME(6) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    raw_factor DECIMAL(18, 9) NULL,
    filtered_factor DECIMAL(18, 9) NULL,
    final_factor DECIMAL(18, 9) NULL,
    funding_signal DECIMAL(18, 9) NULL,
    mr_signal DECIMAL(18, 9) NULL,
    flow_signal DECIMAL(18, 9) NULL,
    vol_regime DECIMAL(18, 9) NULL,
    trend_regime DECIMAL(18, 9) NULL,
    risk_scale DECIMAL(18, 9) NULL,
    extreme_market_factor DECIMAL(18, 9) NULL,
    target_position_usd DECIMAL(20, 8) NULL, -- Calculated desired position value
    UNIQUE KEY unique_symbol_time_signal (symbol, timestamp) -- Avoid duplicate signals per timestamp
);
```

---

**3. `utils/logger.py`**

```python
import logging
import sys

def setup_logger(name="CryptoBot", level=logging.INFO):
    """Sets up a basic logger."""
    logger = logging.getLogger(name)
    if not logger.handlers: # Avoid adding multiple handlers if called multiple times
        logger.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

        # Console Handler
        ch = logging.StreamHandler(sys.stdout)
        ch.setFormatter(formatter)
        logger.addHandler(ch)

        # Optional: File Handler
        # fh = logging.FileHandler('trading_bot.log')
        # fh.setFormatter(formatter)
        # logger.addHandler(fh)

    return logger

# Example usage:
# from utils.logger import setup_logger
# logger = setup_logger(__name__, logging.DEBUG)
# logger.info("This is an info message")
```

---

**4. `utils/helpers.py`**

```python
import pandas as pd
from enum import Enum

class DataLabels(Enum):
    TIMESTAMP = 'timestamp'
    OPEN = 'open'
    HIGH = 'high'
    LOW = 'low'
    CLOSE = 'close'
    VOLUME = 'volume'
    FUNDING = 'funding_rate'
    TAKER_BUY_VOLUME = 'taker_buy_volume' # Need to calculate this
    DOLLAR_VOLUME_DELTA = 'dollar_volume_delta' # Need to calculate this

def calculate_dollar_volume_delta(df):
    """
    Placeholder: Calculates Dollar Volume Delta.
    Requires aggregate trade data (buy/sell volume per trade).
    This simplified version assumes taker_buy_volume is available.
    A proper implementation needs access to individual trade data.
    """
    if DataLabels.TAKER_BUY_VOLUME.value not in df.columns or DataLabels.VOLUME.value not in df.columns:
        # Return zeros or handle missing data appropriately if Taker Buy Volume isn't pre-calculated
        # print(f"Warning: Missing required columns for Dollar Volume Delta calculation.")
        return pd.Series(0.0, index=df.index)

    # Approximate Dollar Volume Delta: (Taker Buy Vol - Taker Sell Vol) * Price
    # Taker Sell Vol = Total Vol - Taker Buy Vol
    taker_sell_volume = df[DataLabels.VOLUME.value] - df[DataLabels.TAKER_BUY_VOLUME.value]
    volume_delta = df[DataLabels.TAKER_BUY_VOLUME.value] - taker_sell_volume
    # Use average price for the period as approximation
    avg_price = (df[DataLabels.OPEN.value] + df[DataLabels.CLOSE.value]) / 2
    dollar_delta = volume_delta * avg_price
    return dollar_delta

def safe_division(numerator, denominator, default=0.0):
    """Performs division, returning default value if denominator is zero or NaN."""
    if isinstance(denominator, (pd.Series, pd.DataFrame)):
        # Handle pandas objects
        result = numerator / denominator.replace(0, 1e-12) # Replace 0 with small number
        result = result.fillna(default) # Fill NaNs resulting from division or original NaNs
        # Handle potential infinities if 1e-12 wasn't enough (though unlikely with floats)
        result = result.replace([float('inf'), -float('inf')], default)
        return result
    else:
        # Handle scalars
        if pd.isna(denominator) or denominator == 0:
            return default
        else:
            # Ensure numerator is also not NaN if it's a scalar
            if pd.isna(numerator):
                return default
            return numerator / denominator

def async_wrap(func):
    """Decorator to run blocking functions in asyncio's default executor."""
    import asyncio
    from functools import wraps, partial

    @wraps(func)
    async def run(*args, loop=None, executor=None, **kwargs):
        if loop is None:
            loop = asyncio.get_event_loop()
        pfunc = partial(func, *args, **kwargs)
        return await loop.run_in_executor(executor, pfunc)
    return run

# Example Usage:
# @async_wrap
# def blocking_db_call(param1):
#    # ... db code ...
#    return result
#
# async def main():
#    res = await blocking_db_call("value")
```

---

**5. `database/db_manager.py`**

```python
import mariadb
import pandas as pd
import logging
import asyncio
from pathlib import Path
from utils.helpers import async_wrap
from utils.logger import setup_logger
from datetime import datetime

logger = setup_logger(__name__)

class DatabaseManager:
    def __init__(self, config):
        self.db_config = config['database']
        self._pool = None # Placeholder for potential future pooling
        self._conn = None
        self._cursor = None
        logger.info("DatabaseManager initialized.")

    @async_wrap
    def _get_connection_sync(self):
        """Synchronous connection function."""
        try:
            conn = mariadb.connect(
                user=self.db_config['user'],
                password=self.db_config['password'],
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database']
            )
            logger.info(f"Successfully connected to MariaDB: {self.db_config['database']}@{self.db_config['host']}")
            return conn
        except mariadb.Error as e:
            logger.error(f"Error connecting to MariaDB: {e}")
            raise # Reraise the exception so the caller knows connection failed

    async def _get_cursor(self):
         """Gets a connection and cursor, creating if necessary."""
         if not self._conn or not self._conn.is_connected():
             try:
                 # Ensure connection is established asynchronously
                 self._conn = await self._get_connection_sync()
                 if self._conn:
                     self._cursor = self._conn.cursor(dictionary=True) # Use dictionary cursor
                 else:
                     # Handle case where connection failed even after async wrap
                     logger.error("Failed to establish database connection asynchronously.")
                     self._cursor = None # Ensure cursor is None if conn is None
                     return None # Indicate failure
             except Exception as e:
                 logger.error(f"Failed to get connection or cursor: {e}")
                 self._conn = None
                 self._cursor = None
                 return None # Indicate failure
         return self._cursor


    @async_wrap
    def _execute_query_sync(self, query, params=None, multi=False):
        """Synchronous query execution."""
        conn = None
        cursor = None
        try:
            # Use the existing connection/cursor logic
            # This sync function needs its own connection temporarily
             conn = mariadb.connect(
                user=self.db_config['user'],
                password=self.db_config['password'],
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database']
            )
             cursor = conn.cursor(dictionary=True)

             if multi:
                 # Handle multi-statement SQL (like CREATE TABLE script)
                 results = []
                 for result in cursor.execute(query, params=params, multi=True):
                     # Process each statement's result if needed, though often not for CREATE
                     # logger.debug(f"Executed: {result.statement}")
                     results.append(cursor.rowcount) # Example: store row counts
                 conn.commit()
                 return results # Or just True/False for success
             else:
                 cursor.execute(query, params)
                 conn.commit()
                 logger.debug(f"Query executed successfully. Rows affected: {cursor.rowcount}")
                 return cursor.rowcount # Return rows affected for INSERT/UPDATE/DELETE
        except mariadb.Error as e:
            logger.error(f"DB Error executing query: {e}\nQuery: {query}\nParams: {params}")
            if conn:
                conn.rollback()
            raise # Re-raise after logging
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()

    @async_wrap
    def _fetch_sync(self, query, params=None, fetch_one=False):
        """Synchronous fetch operation."""
        conn = None
        cursor = None
        try:
            # Needs its own connection for the blocking call within executor
            conn = mariadb.connect(
                user=self.db_config['user'],
                password=self.db_config['password'],
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database']
            )
            cursor = conn.cursor(dictionary=True) # Use dictionary cursor
            cursor.execute(query, params)
            if fetch_one:
                result = cursor.fetchone()
                # logger.debug(f"Fetched one row: {result}")
                return result
            else:
                result = cursor.fetchall()
                # logger.debug(f"Fetched {len(result)} rows.")
                return result
        except mariadb.Error as e:
            logger.error(f"DB Error fetching data: {e}\nQuery: {query}\nParams: {params}")
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()


    async def execute_query(self, query, params=None, multi=False):
        """Executes a query asynchronously."""
        return await self._execute_query_sync(query, params, multi)

    async def fetch_all(self, query, params=None):
        """Fetches all results asynchronously."""
        return await self._fetch_sync(query, params, fetch_one=False)

    async def fetch_one(self, query, params=None):
        """Fetches one result asynchronously."""
        return await self._fetch_sync(query, params, fetch_one=True)

    async def initialize_database(self):
        """Creates tables if they don't exist."""
        sql_file = Path(__file__).parent / 'sql' / 'create_tables.sql'
        try:
            with open(sql_file, 'r') as f:
                sql_script = f.read()
            # MariaDB connector might need specific handling for multi-statement
            # Splitting by semicolon might be fragile if used within statements
            # Using multi=True in execute if supported, or execute statements one by one
            logger.info(f"Initializing database '{self.db_config['database']}'...")
            # The mariadb connector's execute method might support multi=True directly
            await self.execute_query(sql_script, multi=True)
            logger.info("Database initialization script executed successfully.")
        except FileNotFoundError:
            logger.error(f"SQL script not found at {sql_file}")
        except mariadb.Error as e:
            logger.error(f"Failed to initialize database: {e}")
        except Exception as e:
             logger.error(f"An unexpected error occurred during DB initialization: {e}")

    async def close(self):
        """Closes the database connection."""
        # Note: Since we use short-lived connections in the sync wrappers,
        # this might not be strictly necessary unless pooling is implemented.
        if self._conn and self._conn.is_connected():
            try:
                self._conn.close()
                logger.info("Database connection closed.")
            except mariadb.Error as e:
                logger.error(f"Error closing database connection: {e}")
        self._conn = None
        self._cursor = None

    # --- Specific Data Access Methods ---

    async def insert_trade(self, trade_data):
        """Inserts a trade record into the database."""
        query = """
        INSERT INTO trades (symbol, mode, order_id, client_order_id, side, position_side, order_type,
                          price, quantity, quote_quantity, commission, commission_asset,
                          realized_pnl, is_maker, trade_time, strategy_factor, timestamp)
        VALUES (%(symbol)s, %(mode)s, %(order_id)s, %(client_order_id)s, %(side)s, %(position_side)s, %(order_type)s,
                %(price)s, %(quantity)s, %(quote_quantity)s, %(commission)s, %(commission_asset)s,
                %(realized_pnl)s, %(is_maker)s, %(trade_time)s, %(strategy_factor)s, %(timestamp)s)
        """
        try:
            # Ensure all keys exist, providing defaults if necessary
            trade_data.setdefault('order_id', None)
            trade_data.setdefault('client_order_id', None)
            trade_data.setdefault('position_side', 'BOTH')
            trade_data.setdefault('order_type', 'MARKET')
            trade_data.setdefault('commission', 0.0)
            trade_data.setdefault('commission_asset', None)
            trade_data.setdefault('realized_pnl', 0.0)
            trade_data.setdefault('is_maker', None)
            trade_data.setdefault('strategy_factor', None)
            trade_data.setdefault('timestamp', datetime.now()) # Add timestamp if missing

            await self.execute_query(query, trade_data)
            logger.info(f"Trade inserted for {trade_data['symbol']}")
            return True
        except Exception as e:
            logger.error(f"Failed to insert trade for {trade_data.get('symbol', 'N/A')}: {e}")
            return False

    async def update_position(self, position_data):
        """Updates or inserts a position record."""
        query = """
        INSERT INTO positions (symbol, mode, quantity, entry_price, mark_price, unrealized_pnl, leverage, initial_margin, last_update_ts)
        VALUES (%(symbol)s, %(mode)s, %(quantity)s, %(entry_price)s, %(mark_price)s, %(unrealized_pnl)s, %(leverage)s, %(initial_margin)s, NOW(6))
        ON DUPLICATE KEY UPDATE
            quantity = VALUES(quantity),
            entry_price = VALUES(entry_price),
            mark_price = VALUES(mark_price),
            unrealized_pnl = VALUES(unrealized_pnl),
            leverage = VALUES(leverage),
            initial_margin = VALUES(initial_margin),
            last_update_ts = NOW(6)
        """
        try:
             # Ensure all required keys exist
            position_data.setdefault('mark_price', None)
            position_data.setdefault('unrealized_pnl', 0.0)
            position_data.setdefault('initial_margin', 0.0)

            await self.execute_query(query, position_data)
            # logger.debug(f"Position updated for {position_data['symbol']}")
            return True
        except Exception as e:
            logger.error(f"Failed to update position for {position_data.get('symbol', 'N/A')}: {e}")
            return False

    async def get_position(self, symbol, mode):
        """Retrieves a specific position."""
        query = "SELECT * FROM positions WHERE symbol = %s AND mode = %s"
        try:
            return await self.fetch_one(query, (symbol, mode))
        except Exception as e:
            logger.error(f"Failed to get position for {symbol} ({mode}): {e}")
            return None

    async def get_all_positions(self, mode):
        """Retrieves all positions for a given mode."""
        query = "SELECT * FROM positions WHERE mode = %s AND quantity != 0" # Only active positions
        try:
            return await self.fetch_all(query, (mode,))
        except Exception as e:
            logger.error(f"Failed to get all positions for mode {mode}: {e}")
            return []

    async def insert_performance_snapshot(self, performance_data):
        """Inserts a performance snapshot."""
        query = """
        INSERT INTO performance (timestamp, mode, total_equity, realized_pnl_cumulative, unrealized_pnl, total_pnl_cumulative, total_fees_cumulative, trade_count_cumulative)
        VALUES (%(timestamp)s, %(mode)s, %(total_equity)s, %(realized_pnl_cumulative)s, %(unrealized_pnl)s, %(total_pnl_cumulative)s, %(total_fees_cumulative)s, %(trade_count_cumulative)s)
        ON DUPLICATE KEY UPDATE
            total_equity = VALUES(total_equity),
            realized_pnl_cumulative = VALUES(realized_pnl_cumulative),
            unrealized_pnl = VALUES(unrealized_pnl),
            total_pnl_cumulative = VALUES(total_pnl_cumulative),
            total_fees_cumulative = VALUES(total_fees_cumulative),
            trade_count_cumulative = VALUES(trade_count_cumulative)
        """
        try:
            await self.execute_query(query, performance_data)
            # logger.info(f"Performance snapshot inserted for {performance_data['timestamp']} ({performance_data['mode']})")
            return True
        except Exception as e:
            logger.error(f"Failed to insert performance snapshot for {performance_data.get('timestamp', 'N/A')}: {e}")
            return False

    async def get_performance_history(self, mode, limit=1000):
        """Retrieves performance history."""
        query = "SELECT * FROM performance WHERE mode = %s ORDER BY timestamp DESC LIMIT %s"
        try:
            results = await self.fetch_all(query, (mode, limit))
            return results[::-1] # Return in chronological order
        except Exception as e:
            logger.error(f"Failed to get performance history for mode {mode}: {e}")
            return []

    async def get_latest_performance(self, mode):
         """Retrieves the most recent performance snapshot."""
         query = "SELECT * FROM performance WHERE mode = %s ORDER BY timestamp DESC LIMIT 1"
         try:
             return await self.fetch_one(query, (mode,))
         except Exception as e:
             logger.error(f"Failed to get latest performance for mode {mode}: {e}")
             return None


    async def get_trades(self, mode, symbol=None, limit=100):
        """Retrieves trade history."""
        if symbol:
            query = "SELECT * FROM trades WHERE mode = %s AND symbol = %s ORDER BY timestamp DESC LIMIT %s"
            params = (mode, symbol, limit)
        else:
            query = "SELECT * FROM trades WHERE mode = %s ORDER BY timestamp DESC LIMIT %s"
            params = (mode, limit)
        try:
            results = await self.fetch_all(query, params)
            return results # Already ordered DESC
        except Exception as e:
            logger.error(f"Failed to get trades for mode {mode} (Symbol: {symbol}): {e}")
            return []

    async def insert_signal(self, signal_data):
        """Inserts a signal record."""
        query = """
        INSERT INTO signals (timestamp, symbol, raw_factor, filtered_factor, final_factor,
                           funding_signal, mr_signal, flow_signal, vol_regime, trend_regime,
                           risk_scale, extreme_market_factor, target_position_usd)
        VALUES (%(timestamp)s, %(symbol)s, %(raw_factor)s, %(filtered_factor)s, %(final_factor)s,
                %(funding_signal)s, %(mr_signal)s, %(flow_signal)s, %(vol_regime)s, %(trend_regime)s,
                %(risk_scale)s, %(extreme_market_factor)s, %(target_position_usd)s)
        ON DUPLICATE KEY UPDATE
            raw_factor=VALUES(raw_factor), filtered_factor=VALUES(filtered_factor), final_factor=VALUES(final_factor),
            funding_signal=VALUES(funding_signal), mr_signal=VALUES(mr_signal), flow_signal=VALUES(flow_signal),
            vol_regime=VALUES(vol_regime), trend_regime=VALUES(trend_regime), risk_scale=VALUES(risk_scale),
            extreme_market_factor=VALUES(extreme_market_factor), target_position_usd=VALUES(target_position_usd)
        """
        try:
            # Add defaults for optional fields if they aren't calculated/provided
            defaults = {
                'raw_factor': None, 'filtered_factor': None, 'final_factor': None,
                'funding_signal': None, 'mr_signal': None, 'flow_signal': None,
                'vol_regime': None, 'trend_regime': None, 'risk_scale': None,
                'extreme_market_factor': None, 'target_position_usd': None
            }
            for key, value in defaults.items():
                signal_data.setdefault(key, value)

            await self.execute_query(query, signal_data)
            # logger.debug(f"Signal inserted/updated for {signal_data['symbol']} at {signal_data['timestamp']}")
            return True
        except Exception as e:
            logger.error(f"Failed to insert signal for {signal_data.get('symbol', 'N/A')}: {e}")
            logger.error(f"Signal Data: {signal_data}")
            return False

    async def get_last_signal_factor(self, symbol, mode):
        """ Gets the last recorded final_factor for turnover reduction logic. """
        # This might need adjustment based on how signals are stored and used
        query = "SELECT final_factor, timestamp FROM signals WHERE symbol = %s ORDER BY timestamp DESC LIMIT 1"
        # Assuming mode isn't stored in signals, might need adjustment if it is
        try:
            result = await self.fetch_one(query, (symbol,))
            return result['final_factor'] if result else 0.0 # Return 0 if no previous signal
        except Exception as e:
            logger.error(f"Failed to get last signal factor for {symbol}: {e}")
            return 0.0


```

---

**6. `interfaces/binance_interface.py`**

```python
import asyncio
import time
import hmac
import hashlib
from binance import AsyncClient, BinanceSocketManager
from binance.exceptions import BinanceAPIException, BinanceRequestException
from utils.logger import setup_logger
from urllib.parse import urlencode
import pandas as pd
from datetime import datetime, timedelta
import math

logger = setup_logger(__name__)

# Define base URLs
BINANCE_FUTURES_URL = "https://fapi.binance.com"
BINANCE_FUTURES_TESTNET_URL = "https://testnet.binancefuture.com"

class BinanceInterface:
    def __init__(self, config):
        self.config = config
        self.api_key = None
        self.api_secret = None
        self.client = None
        self.bsm = None
        self.trading_mode = config['trading_mode']
        self.kline_interval = config['kline_interval']
        self.futures_symbols = [] # List of actively traded USDT perpetual symbols
        self._initialize_credentials()

    def _initialize_credentials(self):
        """Loads API keys based on trading mode."""
        binance_config = self.config['binance']
        if self.trading_mode == 'live':
            self.api_key = binance_config.get('api_key')
            self.api_secret = binance_config.get('api_secret')
            base_url = BINANCE_FUTURES_URL
            logger.info("Initializing Binance Interface for LIVE trading.")
        elif self.trading_mode == 'testnet':
            self.api_key = binance_config.get('testnet_api_key')
            self.api_secret = binance_config.get('testnet_api_secret')
            base_url = BINANCE_FUTURES_TESTNET_URL
            logger.info("Initializing Binance Interface for TESTNET trading.")
        else: # paper mode
            self.api_key = "PAPER_KEY" # Dummy values for paper trading
            self.api_secret = "PAPER_SECRET"
            base_url = BINANCE_FUTURES_URL # Use live URL for market data in paper mode
            logger.info("Initializing Binance Interface for PAPER trading (using live market data).")

        if (self.trading_mode == 'live' or self.trading_mode == 'testnet') and (not self.api_key or not self.api_secret):
            logger.warning(f"API Key or Secret not found for {self.trading_mode} mode in config. Trading actions will fail.")
            # Optionally raise an error if keys are strictly required
            # raise ValueError(f"API Key/Secret missing for {self.trading_mode}")

        # Store the base URL for potential direct requests if needed
        self.base_url = base_url


    async def initialize_client(self):
        """Initializes the AsyncClient and BinanceSocketManager."""
        if self.client:
            await self.close_client() # Close existing client if any

        testnet = (self.trading_mode == 'testnet')
        try:
            self.client = await AsyncClient.create(self.api_key, self.api_secret, testnet=testnet)
            self.bsm = BinanceSocketManager(self.client)
            logger.info(f"AsyncClient and BinanceSocketManager initialized for {'Testnet' if testnet else 'Live/Paper'}.")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Binance client: {e}")
            self.client = None
            self.bsm = None
            return False

    async def close_client(self):
        """Closes the client session and socket manager."""
        if self.bsm and self.bsm.socket:
             logger.info("Closing Binance WebSocket Manager...")
             # The socket manager doesn't have an explicit async close method in some versions
             # Usually closing the client handles underlying connections
             pass # await self.bsm.close() # If available
        if self.client:
            logger.info("Closing Binance AsyncClient session...")
            await self.client.close_connection()
            logger.info("Binance AsyncClient session closed.")
        self.client = None
        self.bsm = None

    async def _make_request(self, method, *args, **kwargs):
        """Wrapper for API calls with error handling and retries."""
        if not self.client and self.trading_mode != 'paper':
             logger.error("Binance client not initialized. Cannot make API request.")
             # Re-initialize if possible, or raise error
             if not await self.initialize_client():
                 raise ConnectionError("Failed to re-initialize Binance client.")

        # Paper mode doesn't need client for placing orders etc.
        if self.trading_mode == 'paper' and method.__name__ not in ['get_klines', 'get_funding_rate_history', 'get_exchange_info', 'get_ticker']:
             logger.debug(f"Paper mode: Skipping API call {method.__name__}")
             return None # Or simulate response if needed

        retries = 3
        for i in range(retries):
            try:
                # logger.debug(f"Calling API method: {method.__name__} with args: {args}, kwargs: {kwargs}")
                return await method(*args, **kwargs)
            except BinanceAPIException as e:
                logger.error(f"Binance API Error on {method.__name__}: {e}")
                if e.code == -1021: # Timestamp error
                    logger.warning("Timestamp error detected, consider syncing system clock.")
                    # Potentially adjust request timestamp or wait
                # Add handling for other specific API errors if needed
                await asyncio.sleep(2 ** i) # Exponential backoff
            except BinanceRequestException as e:
                logger.error(f"Binance Request Error on {method.__name__}: {e}")
                await asyncio.sleep(2 ** i)
            except ConnectionError as e:
                 logger.error(f"Connection Error on {method.__name__}: {e}")
                 await asyncio.sleep(5) # Longer wait for connection issues
                 # Attempt to re-initialize client on connection error
                 logger.info("Attempting to re-initialize Binance client due to ConnectionError...")
                 await self.initialize_client()
            except Exception as e: # Catch broader exceptions
                logger.error(f"Unexpected Error on {method.__name__}: {e}")
                # Don't retry for unexpected errors immediately unless known safe
                raise # Re-raise unexpected errors after logging
        logger.error(f"API call {method.__name__} failed after {retries} retries.")
        return None # Indicate failure after retries

    async def get_usdt_futures_symbols(self, only_perpetual=True):
        """Fetches all USDT-margined futures symbols, optionally filtering for perpetuals."""
        try:
            exchange_info = await self._make_request(self.client.futures_exchange_info)
            if not exchange_info: return []

            symbols = []
            for item in exchange_info['symbols']:
                is_usdt_margined = item['marginAsset'] == 'USDT' and item['quoteAsset'] == 'USDT'
                is_perpetual = item['contractType'] == 'PERPETUAL'
                is_trading = item['status'] == 'TRADING'

                if is_usdt_margined and is_trading:
                    if only_perpetual and is_perpetual:
                        symbols.append(item['symbol'])
                    elif not only_perpetual:
                        symbols.append(item['symbol'])

            self.futures_symbols = symbols
            logger.info(f"Fetched {len(symbols)} USDT perpetual futures symbols.")
            return symbols
        except Exception as e:
            logger.error(f"Failed to fetch futures symbols: {e}")
            return []

    async def get_historical_klines(self, symbol, interval, start_str=None, end_str=None, limit=500):
        """Fetches historical klines."""
        try:
            # Ensure limit is within Binance's max (e.g., 1500 for futures)
            limit = min(limit, 1500)
            klines = await self._make_request(
                self.client.futures_get_klines,
                symbol=symbol,
                interval=interval,
                startTime=start_str, # Optional: String format "YYYY-MM-DD HH:MM:SS" or ms timestamp
                endTime=end_str,     # Optional
                limit=limit
            )
            if not klines: return pd.DataFrame()

            # Convert to DataFrame
            df = pd.DataFrame(klines, columns=[
                'kline_open_time', 'open', 'high', 'low', 'close', 'volume',
                'kline_close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])

            # Convert timestamps to datetime and numeric types
            df['timestamp'] = pd.to_datetime(df['kline_open_time'], unit='ms')
            numeric_cols = ['open', 'high', 'low', 'close', 'volume',
                            'quote_asset_volume', 'taker_buy_base_asset_volume',
                            'taker_buy_quote_asset_volume']
            for col in numeric_cols:
                df[col] = pd.to_numeric(df[col])

            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'taker_buy_base_asset_volume']]
            df.rename(columns={'taker_buy_base_asset_volume': 'taker_buy_volume'}, inplace=True)
            df.set_index('timestamp', inplace=True)

            # logger.debug(f"Fetched {len(df)} klines for {symbol} interval {interval}")
            return df

        except Exception as e:
            logger.error(f"Failed to fetch historical klines for {symbol}: {e}")
            return pd.DataFrame()

    async def get_funding_rate_history(self, symbol, limit=500):
         """ Fetches historical funding rates. Note: API limit might be less than klines. """
         try:
             # Binance API limit for funding rate is typically 1000 max, default might be 100
             limit = min(limit, 1000)
             funding_data = await self._make_request(
                 self.client.futures_funding_rate,
                 symbol=symbol,
                 limit=limit
             )
             if not funding_data: return pd.DataFrame()

             df = pd.DataFrame(funding_data)
             df['timestamp'] = pd.to_datetime(df['fundingTime'], unit='ms')
             df['funding_rate'] = pd.to_numeric(df['fundingRate'])
             df = df[['timestamp', 'funding_rate']].set_index('timestamp')

             # Funding rates are typically every 8 hours. We might need to forward-fill
             # logger.debug(f"Fetched {len(df)} funding rate entries for {symbol}")
             return df

         except Exception as e:
             logger.error(f"Failed to fetch funding rate history for {symbol}: {e}")
             return pd.DataFrame()


    async def start_kline_websockets(self, symbols, interval, callback):
        """Starts kline websockets for given symbols."""
        if not self.bsm:
            logger.error("BinanceSocketManager not initialized. Cannot start websockets.")
            return None

        logger.info(f"Starting kline websockets for {len(symbols)} symbols with interval {interval}...")
        streams = [f"{s.lower()}@kline_{interval}" for s in symbols]

        try:
            socket = self.bsm.multiplex_socket(streams)
            logger.info(f"Multiplex socket created for streams: {streams}")

            async def socket_processor():
                async with socket as stream:
                    while True:
                        try:
                            res = await stream.recv()
                            # logger.debug(f"WS Received: {res}") # Very verbose
                            if res and 'stream' in res and 'data' in res:
                                # Check for error messages
                                if 'e' in res['data'] and res['data']['e'] == 'error':
                                     logger.error(f"WebSocket error message received: {res['data']}")
                                     continue # Skip processing this message

                                # Check if it's kline data and has the kline structure
                                if 'k' in res['data']:
                                     await callback(res['data']) # Pass the 'data' part to the callback
                                else:
                                     logger.warning(f"Received non-kline message structure: {res['data']}")

                            elif res is None:
                                # Handle potential socket closure or empty message
                                logger.warning("Received None from websocket stream. Attempting to reconnect or handle.")
                                # Implement reconnection logic if necessary here or in the main loop
                                await asyncio.sleep(5) # Wait before potentially trying again
                            else:
                                logger.warning(f"Received unexpected message format: {res}")

                        except ConnectionClosedOK:
                             logger.warning("WebSocket connection closed normally.")
                             break # Exit the loop if connection is closed
                        except ConnectionClosedError as e:
                            logger.error(f"WebSocket connection closed with error: {e}. Attempting reconnect...")
                            # Implement more robust reconnection logic here
                            await asyncio.sleep(10)
                            # Need to restart the socket connection process
                            # This might require signaling the main loop to re-initiate
                            # For now, just log and break
                            break
                        except Exception as e:
                            logger.error(f"Error processing websocket message: {e}")
                            # Decide whether to continue or break based on error severity
                            await asyncio.sleep(1) # Avoid tight loop on errors

            # Run the processor in the background
            asyncio.create_task(socket_processor())
            logger.info("Kline websocket processor task created.")
            return socket # Return the socket object if needed for external management

        except Exception as e:
            logger.error(f"Failed to start kline websockets: {e}")
            return None

    async def place_order(self, symbol, side, quantity, order_type='MARKET', position_side='BOTH', time_in_force=None, price=None, new_client_order_id=None):
        """Places an order."""
        if self.trading_mode == 'paper':
            logger.info(f"[Paper Trade] Order: {side} {quantity} {symbol} @ MARKET")
            # Simulate a market order fill
            # For paper trading, we need the current price elsewhere
            # This function in paper mode might just log, execution happens in Executor
            return {
                'orderId': f'PAPER_{int(time.time() * 1000)}',
                'symbol': symbol,
                'status': 'FILLED', # Assume immediate fill for paper market orders
                'clientOrderId': new_client_order_id or f'paper_{symbol}_{int(time.time())}',
                'side': side.upper(),
                'type': order_type.upper(),
                'origQty': f"{quantity:.8f}", # Use string format like Binance API
                'executedQty': f"{quantity:.8f}",
                 # Price needs to be determined by Executor using last known price
                'avgPrice': '0.0', # Placeholder, executor should fill this
                'timeInForce': time_in_force or 'GTC',
                'positionSide': position_side,
                'reduceOnly': False, # Assuming not reduceOnly unless specified
                'cumQuote': '0.0', # Placeholder
                'activatePrice': None, # For stop/take profit
                'priceRate': None, # For trailing stop
                 'updateTime': int(time.time() * 1000),
                 'workingType': 'CONTRACT_PRICE',
                 'priceProtect': False,
                 'origType': order_type.upper(),
                 'is_paper': True # Flag for paper trade
            }

        if not self.client:
             logger.error("Binance client not initialized. Cannot place order.")
             raise ConnectionError("Binance client not available.")

        params = {
            'symbol': symbol,
            'side': side.upper(), # 'BUY' or 'SELL'
            'positionSide': position_side.upper(), # 'BOTH', 'LONG', 'SHORT'
            'type': order_type.upper(), # 'MARKET', 'LIMIT', etc.
            'quantity': f"{quantity:.8f}", # Ensure quantity is formatted correctly as string
        }
        if order_type.upper() == 'LIMIT':
            if price is None:
                raise ValueError("Price must be specified for LIMIT orders.")
            params['price'] = f"{price:.8f}" # Format price as string
            params['timeInForce'] = time_in_force or 'GTC' # Good Till Cancelled is common for LIMIT

        if new_client_order_id:
            params['newClientOrderId'] = new_client_order_id

        logger.info(f"Placing Order: {params}")
        try:
            order = await self._make_request(self.client.futures_create_order, **params)
            logger.info(f"Order placed successfully: {order}")
            return order
        except Exception as e:
            logger.error(f"Failed to place order for {symbol}: {e}")
            # Return a structure indicating failure, or re-raise
            return {'symbol': symbol, 'status': 'FAILED', 'error': str(e)}

    async def get_position_information(self, symbol=None):
        """Gets position information for one or all symbols."""
        if self.trading_mode == 'paper':
            logger.debug("Paper mode: Skipping get_position_information API call.")
            # Position info should be managed internally in paper mode
            return [] # Or fetch from internal state if needed

        try:
            params = {'symbol': symbol} if symbol else {}
            positions = await self._make_request(self.client.futures_position_information, **params)
            # logger.debug(f"Fetched position information: {positions}")
             # Filter out positions with zero amount if needed (Binance returns all potential symbols)
            # active_positions = [p for p in positions if float(p.get('positionAmt', 0)) != 0]
            # return active_positions
            return positions # Return all for potential analysis
        except Exception as e:
            logger.error(f"Failed to get position information for {symbol or 'all symbols'}: {e}")
            return []

    async def get_account_balance(self):
         """Gets futures account balance."""
         if self.trading_mode == 'paper':
             logger.debug("Paper mode: Skipping get_account_balance API call.")
             # Balance managed internally for paper mode
             return []

         try:
             balance = await self._make_request(self.client.futures_account_balance)
             # logger.debug(f"Fetched account balance: {balance}")
             return balance # Returns a list of assets
         except Exception as e:
             logger.error(f"Failed to get account balance: {e}")
             return []

    async def change_leverage(self, symbol, leverage):
        """Changes leverage for a symbol."""
        if self.trading_mode == 'paper':
            logger.info(f"[Paper Mode] Set leverage for {symbol} to {leverage}")
            # In paper mode, just record the intent or store it internally
            return {'symbol': symbol, 'leverage': leverage, 'status': 'PAPER_SET'}

        logger.info(f"Changing leverage for {symbol} to {leverage}x")
        try:
            response = await self._make_request(self.client.futures_change_leverage, symbol=symbol, leverage=leverage)
            logger.info(f"Leverage changed successfully for {symbol}: {response}")
            return response
        except BinanceAPIException as e:
             # Handle specific errors like "Leverage not modified" which might be ok
             if e.code == -4046: # "Leverage not modified" code
                 logger.warning(f"Leverage for {symbol} already set to {leverage}. Code: {e.code}")
                 return {'symbol': symbol, 'leverage': leverage, 'status': 'NOT_MODIFIED'}
             else:
                 logger.error(f"API Error changing leverage for {symbol} to {leverage}: {e}")
                 raise # Reraise other API errors
        except Exception as e:
            logger.error(f"Failed to change leverage for {symbol} to {leverage}: {e}")
            raise # Reraise unexpected errors

    async def get_ticker(self, symbol):
         """Gets the latest price ticker for a symbol."""
         try:
             ticker = await self._make_request(self.client.futures_ticker, symbol=symbol)
             if ticker and 'lastPrice' in ticker:
                 return {'symbol': symbol, 'price': float(ticker['lastPrice']), 'timestamp': datetime.now()}
             else:
                 logger.warning(f"Could not get valid ticker for {symbol}. Response: {ticker}")
                 return None
         except Exception as e:
             logger.error(f"Failed to get ticker for {symbol}: {e}")
             return None


    # --- Helper to get multiple data types efficiently ---
    async def fetch_initial_data(self, symbol, interval, limit):
        """Fetches klines and funding rates for initialization."""
        klines_task = self.get_historical_klines(symbol, interval, limit=limit)
        funding_task = self.get_funding_rate_history(symbol, limit=limit) # Match limit roughly

        try:
            klines_df, funding_df = await asyncio.gather(klines_task, funding_task)

            if klines_df.empty:
                logger.warning(f"No kline data returned for {symbol}")
                return None

            # Combine dataframes
            # Funding data is sparse (e.g., 8hr intervals)
            # Forward fill funding rate onto the kline intervals
            combined_df = klines_df.copy()

            if not funding_df.empty:
                # Ensure funding_df index is datetime
                funding_df.index = pd.to_datetime(funding_df.index)
                 # Reindex funding to match klines and forward fill
                funding_reindexed = funding_df.reindex(combined_df.index, method='ffill')
                combined_df[DataLabels.FUNDING.value] = funding_reindexed['funding_rate']
                 # Fill initial NaNs if klines start before first funding point
                combined_df[DataLabels.FUNDING.value].fillna(method='bfill', inplace=True) # Backfill first
                combined_df[DataLabels.FUNDING.value].fillna(0, inplace=True) # Fill remaining if any with 0
            else:
                 logger.warning(f"No funding rate data for {symbol}, filling with 0.")
                 combined_df[DataLabels.FUNDING.value] = 0.0 # Set default if no funding data

            # Calculate Dollar Volume Delta (Placeholder/Approximation)
            from utils.helpers import calculate_dollar_volume_delta, DataLabels
            combined_df[DataLabels.DOLLAR_VOLUME_DELTA.value] = calculate_dollar_volume_delta(combined_df)


            # logger.info(f"Successfully combined initial data for {symbol}. Shape: {combined_df.shape}")
            return combined_df

        except Exception as e:
            logger.error(f"Error during initial data fetch for {symbol}: {e}")
            return None

```

---

**7. `strategy/strategy_v2.py`**

```python
import pandas as pd
import numpy as np
from utils.helpers import DataLabels, safe_division
from utils.logger import setup_logger

logger = setup_logger(__name__)

class TradingStrategyV2:
    def __init__(self, config):
        self.params = config.get('strategy_v2', {}) # Load params from config if available
        # Set defaults if not in config, matching the description
        self.lookback_long = self.params.get('lookback_long', 240)
        self.lookback_medium = self.params.get('lookback_medium', 120)
        self.lookback_short = self.params.get('lookback_short', 24)
        self.funding_roll = self.params.get('funding_roll', 144)
        self.mr_roll = self.params.get('mr_roll', 96)
        self.flow_roll = self.params.get('flow_roll', 120)
        self.perf_roll = self.params.get('perf_roll', 192)
        self.perf_smooth_span = self.params.get('perf_smooth_span', 60)

        # Base weights
        self.funding_weight_base = self.params.get('funding_weight_base', 0.5)
        self.mr_weight_base = self.params.get('mr_weight_base', 0.3)
        self.flow_weight_base = self.params.get('flow_weight_base', 0.2)

        # Store previous factors for change threshold filter (optional, managed by TradingEngine state now)
        # self.previous_factors = {}

        logger.info("TradingStrategyV2 initialized with parameters.")
        logger.debug(f"Strategy Params: {self.__dict__}")


    def calculate_signals(self, symbol: str, df: pd.DataFrame):
        """
        Calculates all strategy components and the final factor for a single symbol's data.

        Args:
            symbol: The trading symbol (e.g., 'BTCUSDT').
            df: A pandas DataFrame with columns defined in DataLabels
                (timestamp index, open, high, low, close, volume,
                 funding_rate, taker_buy_volume, dollar_volume_delta).

        Returns:
            A dictionary containing the latest calculated values for:
            'timestamp', 'symbol', 'raw_factor', 'filtered_factor', 'final_factor',
            and intermediate signals for analysis/debugging, or None if calculation fails.
        """
        if df.empty or len(df) < self.lookback_long:
            logger.warning(f"Insufficient data for {symbol} to calculate signals. Need {self.lookback_long}, have {len(df)}")
            return None

        try:
            # Make a copy to avoid modifying the original DataFrame from DataHandler
            data = df.copy()
            min_std_dev = 1e-8 # Minimum std dev to avoid division by zero

            # ------ MARKET REGIME DETECTION ------
            returns = data[DataLabels.CLOSE.value].pct_change(self.lookback_short) # Use 24 period return
            vol = returns.rolling(72).std().clip(lower=min_std_dev) # Clip std dev lower bound
            vol_regime = (safe_division(vol, vol.rolling(self.lookback_long).mean()) - 1).clip(-2, 2)
            vol_regime_smooth = vol_regime.ewm(span=36, adjust=False).mean()

            ma_fast = data[DataLabels.CLOSE.value].rolling(self.lookback_short).mean() # 24 period MA
            ma_slow = data[DataLabels.CLOSE.value].rolling(168).mean() # 168 period (1 week) MA
            trend_ratio = safe_division(ma_fast, ma_slow) - 1
            trend_regime = trend_ratio.clip(-0.10, 0.10).ewm(span=self.lookback_short, adjust=False).mean() # Use 24 span

            return_vol_ratio = safe_division(returns, vol.shift(1)) # Normalized returns
            extreme_moves = (return_vol_ratio.abs() > 2.5).astype(float)
            extreme_market = (extreme_moves.rolling(48).sum() > 2).astype(float) # Ensure it's float for multiplication
            extreme_market_factor = (1 - extreme_market * 0.7).clip(0.3, 1)

            # ------ FUNDING RATE COMPONENT ------
            funding = data[DataLabels.FUNDING.value] # Assuming pre-filled funding rate column
            funding_mean = funding.rolling(self.funding_roll).mean()
            funding_std = funding.rolling(self.funding_roll).std().clip(lower=min_std_dev)
            funding_z = safe_division(funding - funding_mean, funding_std)
            funding_signal_raw = -1 * funding_z # Inverse relationship

            funding_threshold = funding_z.abs().rolling(self.lookback_long).quantile(0.70)
            funding_signal_filtered = funding_signal_raw * (funding_signal_raw.abs() > funding_threshold).astype(float)
            # Persistence filter (using EWM directly on the filtered signal)
            funding_signal = funding_signal_filtered.ewm(span=18, adjust=False).mean()


            # ------ SHORT-TERM MEAN REVERSION COMPONENT ------
            returns_z = safe_division(returns, returns.rolling(self.mr_roll).std().clip(lower=min_std_dev))
            # Adaptive threshold based on smoothed volatility regime
            mean_rev_threshold = 1.8 + 0.3 * vol_regime_smooth.abs()
            mean_rev_raw = -1 * returns_z * (returns_z.abs() > mean_rev_threshold).astype(float)

            # Trend filter: Reduce MR signal strength in strong trends
            trend_filter_mr = (1.0 - (trend_regime.abs() * 5.0).clip(0, 0.8))
            mean_rev_trend_filtered = mean_rev_raw * trend_filter_mr

            # Volume confirmation
            volume_mean = data[DataLabels.VOLUME.value].rolling(72).mean()
            volume_ratio = safe_division(data[DataLabels.VOLUME.value], volume_mean)
            volume_filter = (volume_ratio > 0.8).astype(float)
            mean_rev_signal = mean_rev_trend_filtered * volume_filter


            # ------ SMART MONEY FLOW COMPONENT ------
            # Ensure Taker Buy Volume and Dollar Volume Delta are present
            if DataLabels.TAKER_BUY_VOLUME.value not in data.columns or DataLabels.DOLLAR_VOLUME_DELTA.value not in data.columns:
                 logger.warning(f"Missing Taker Buy Volume or Dollar Volume Delta for {symbol}. Flow signal will be zero.")
                 flow_signal = pd.Series(0.0, index=data.index)
            else:
                buy_ratio = safe_division(data[DataLabels.TAKER_BUY_VOLUME.value], data[DataLabels.VOLUME.value].clip(lower=min_std_dev))
                buy_ratio_mean = buy_ratio.rolling(self.flow_roll).mean()
                buy_ratio_std = buy_ratio.rolling(self.flow_roll).std().clip(lower=min_std_dev)
                buy_ratio_z = safe_division(buy_ratio - buy_ratio_mean, buy_ratio_std)

                dollar_delta = data[DataLabels.DOLLAR_VOLUME_DELTA.value]
                dollar_delta_mean = dollar_delta.rolling(self.flow_roll).mean()
                dollar_delta_std = dollar_delta.rolling(self.flow_roll).std().clip(lower=min_std_dev)
                dollar_delta_z = safe_division(dollar_delta - dollar_delta_mean, dollar_delta_std)

                flow_combined = (buy_ratio_z * 0.7 + dollar_delta_z * 0.3).ewm(span=12, adjust=False).mean()
                flow_threshold = flow_combined.abs().rolling(self.lookback_long).quantile(0.75)
                flow_signal_filtered = flow_combined * (flow_combined.abs() > flow_threshold).astype(float)

                # Volume scaling (less aggressive)
                volume_z_mean = data[DataLabels.VOLUME.value].rolling(self.flow_roll).mean()
                volume_z_std = data[DataLabels.VOLUME.value].rolling(self.flow_roll).std().clip(lower=min_std_dev)
                volume_z = safe_division(data[DataLabels.VOLUME.value] - volume_z_mean, volume_z_std)
                volume_scale = (1.0 + 0.2 * volume_z).clip(0.8, 1.3)
                flow_signal = flow_signal_filtered * volume_scale


            # ------ DYNAMIC WEIGHTING SYSTEM ------
            # Calculate lagged returns for performance calculation
            returns_shifted = returns # Using current returns to calculate performance based on *previous* signal

            # Calculate rolling performance (use .mean() directly)
            funding_perf = (funding_signal.shift(1) * returns_shifted).rolling(self.perf_roll).mean().fillna(0)
            mr_perf = (mean_rev_signal.shift(1) * returns_shifted).rolling(self.perf_roll).mean().fillna(0)
            flow_perf = (flow_signal.shift(1) * returns_shifted).rolling(self.perf_roll).mean().fillna(0)

            # Smoother performance tracking
            funding_perf_smooth = funding_perf.ewm(span=self.perf_smooth_span, adjust=False).mean()
            mr_perf_smooth = mr_perf.ewm(span=self.perf_smooth_span, adjust=False).mean()
            flow_perf_smooth = flow_perf.ewm(span=self.perf_smooth_span, adjust=False).mean()

            # Calculate adjustment factor (Reduced magnitude)
            perf_adj_base = 0.15
            perf_sum = (funding_perf_smooth.abs() + mr_perf_smooth.abs() + flow_perf_smooth.abs()).clip(lower=min_std_dev) # Add abs() and clip

            # Performance-based weight adjustments
            funding_weight_adj = safe_division(perf_adj_base * funding_perf_smooth, perf_sum)
            mr_weight_adj = safe_division(perf_adj_base * mr_perf_smooth, perf_sum)
            flow_weight_adj = safe_division(perf_adj_base * flow_perf_smooth, perf_sum)

            # Apply adjustments and clip weights
            funding_weight = (self.funding_weight_base + funding_weight_adj).clip(0.4, 0.6)
            mr_weight = (self.mr_weight_base + mr_weight_adj).clip(0.25, 0.4)
            flow_weight = (self.flow_weight_base + flow_weight_adj).clip(0.15, 0.25)

            # Normalize weights to ensure they sum to 1
            weight_sum = funding_weight + mr_weight + flow_weight
            funding_weight_norm = safe_division(funding_weight, weight_sum)
            mr_weight_norm = safe_division(mr_weight, weight_sum)
            flow_weight_norm = safe_division(flow_weight, weight_sum)

            # ------ COMBINE COMPONENTS AND APPLY FILTERS ------
            raw_factor = (
                funding_signal * funding_weight_norm +
                mean_rev_signal * mr_weight_norm +
                flow_signal * flow_weight_norm
            ).fillna(0) # Ensure no NaNs in raw factor

            # ------ POSITION SIZING AND TAIL RISK MANAGEMENT ------ (Applied before turnover reduction)
            # Drawdown calculation (relative to rolling max)
            rolling_max_price = data[DataLabels.CLOSE.value].rolling(self.lookback_long).max()
            price_drawdown = safe_division(data[DataLabels.CLOSE.value], rolling_max_price) - 1
            # Normalize drawdown severity (0=no drawdown, 1=25% drawdown or more)
            drawdown_severity = (price_drawdown.clip(-0.25, 0) / -0.25).fillna(0)

            # Risk scaling factors
            vol_risk_scale = (1.0 / (1.0 + vol_regime_smooth.abs() * 0.8)).clip(0.4, 1.2)
            drawdown_scale = (1.0 - drawdown_severity * 0.6).clip(0.5, 1.0)

            # Combined risk scale (simple average as per description)
            # Original: risk_scale = vol_risk_scale * 0.7 + drawdown_scale * 0.3
            # Let's use the weighted average from the description
            risk_scale = vol_risk_scale * 0.7 + drawdown_scale * 0.3
            # Ensure risk_scale is not NaN, default to 1 if issues
            risk_scale = risk_scale.fillna(1.0)


            # Apply asymmetric trend filter
            trend_threshold = 0.02
            trend_filter = pd.Series(1.0, index=data.index)
            # Reduce shorts in uptrend: If trend > threshold AND raw_factor < 0 (short signal)
            trend_filter = trend_filter - ((trend_regime > trend_threshold) * (trend_regime * 3.0) * (raw_factor < 0)).astype(float)
            # Reduce longs in downtrend: If trend < -threshold AND raw_factor > 0 (long signal)
            # Note: trend_regime is negative here, so subtracting a negative * negative = adding reduction
            trend_filter = trend_filter - ((trend_regime < -trend_threshold) * (trend_regime.abs() * 3.0) * (raw_factor > 0)).astype(float)
            # Clip filter result just in case
            trend_filter = trend_filter.clip(0, 1) # Ensure filter is between 0 and 1


            # Apply filters: Trend, Risk Scale, Extreme Market
            factor_with_filters = raw_factor * trend_filter * risk_scale * extreme_market_factor


            # ------ TURNOVER REDUCTION AND SIGNAL REFINEMENT ------
            # Conviction filtering
            signal_thresh_val = factor_with_filters.abs().rolling(self.lookback_long).quantile(0.75).fillna(min_std_dev) # Use long lookback quantile
            # Avoid division by zero or near-zero threshold
            conviction_scale = (safe_division(factor_with_filters.abs(), signal_thresh_val.clip(lower=min_std_dev)) ** 1.5).clip(0, 1.5)
            conviction_factor = factor_with_filters * conviction_scale

            # Time-based signal decay (Applied only if signal drops below threshold)
            signal_decay = 0.85
            # Threshold check should be based on the conviction_factor's strength relative to its threshold
            # Let's use the `signal_thresh_val` calculated earlier for consistency
            signal_mask = (conviction_factor.abs() > signal_thresh_val).astype(float)

            # Need to apply decay iteratively or using a trick, EWM is simpler for smoothing decay effects
            # Simpler approach: Use EWM smoothing after conviction scaling instead of complex decay logic
            # decayed_factor = conviction_factor * signal_mask + conviction_factor.shift(1).fillna(0) * signal_decay * (1 - signal_mask) # This requires careful state management or iteration

            # Additional smoothing (applied to the conviction-scaled factor)
            smoothed_factor = conviction_factor.ewm(span=18, adjust=False).mean()

            # Final volatility standardization
            factor_std = smoothed_factor.rolling(self.lookback_medium).std().clip(lower=min_std_dev) # Use 120 period std dev
            target_vol = self.params.get('target_signal_volatility', 0.2) # Get target vol from config/defaults
            factor_standardized = safe_division(smoothed_factor, factor_std) * target_vol

            # Apply final change threshold filter (Logic moved to TradingEngine for statefulness)
            # This requires knowing the *previous* final factor value
            # change_threshold_val = factor_standardized.rolling(self.lookback_short).std() * 0.4 # 24 period std * 0.4
            # factor_change = factor_standardized - factor_standardized.shift(1).fillna(0) # Calculate change from previous factor
            # final_factor = factor_standardized * (factor_change.abs() > change_threshold_val).astype(float) + \
            #                factor_standardized.shift(1).fillna(0) * (factor_change.abs() <= change_threshold_val).astype(float)

            final_factor = factor_standardized # Renamed for clarity, change filter applied externally


            # Return the latest values as a dictionary
            latest_idx = data.index[-1]
            result = {
                "timestamp": latest_idx,
                "symbol": symbol,
                "raw_factor": raw_factor.loc[latest_idx],
                "filtered_factor": factor_with_filters.loc[latest_idx], # Factor after trend/risk filters
                "final_factor": final_factor.loc[latest_idx], # Factor after all smoothing/scaling (before change threshold)
                # Intermediate signals for debugging/analysis
                "funding_signal": funding_signal.loc[latest_idx],
                "mr_signal": mean_rev_signal.loc[latest_idx],
                "flow_signal": flow_signal.loc[latest_idx],
                "vol_regime": vol_regime_smooth.loc[latest_idx],
                "trend_regime": trend_regime.loc[latest_idx],
                "risk_scale": risk_scale.loc[latest_idx],
                "extreme_market_factor": extreme_market_factor.loc[latest_idx],
                # Add weights for analysis
                "funding_weight": funding_weight_norm.loc[latest_idx],
                 "mr_weight": mr_weight_norm.loc[latest_idx],
                 "flow_weight": flow_weight_norm.loc[latest_idx],
            }

            # Replace any potential NaN/inf values with 0 before returning
            for key, value in result.items():
                 if isinstance(value, (int, float)):
                     if not np.isfinite(value):
                         logger.warning(f"Non-finite value detected in final signal result for {symbol}: {key}={value}. Replacing with 0.")
                         result[key] = 0.0
                 elif key == "timestamp":
                     continue # Skip timestamp
                 elif value is None: # Handle None as well
                      result[key] = 0.0


            # logger.debug(f"Signal calculation successful for {symbol} at {latest_idx}. Final Factor: {result['final_factor']:.4f}")
            return result

        except KeyError as e:
            logger.error(f"Missing expected column in DataFrame for {symbol}: {e}")
            return None
        except Exception as e:
            logger.exception(f"Error calculating signals for {symbol}: {e}") # Use exception to include traceback
            return None

```

---

**8. `core/data_handler.py`**

```python
import asyncio
import pandas as pd
from collections import deque
from interfaces.binance_interface import BinanceInterface
from utils.logger import setup_logger
from utils.helpers import DataLabels, calculate_dollar_volume_delta
from datetime import datetime, timedelta, timezone
import time

logger = setup_logger(__name__)

class DataHandler:
    def __init__(self, config, binance_interface: BinanceInterface, db_manager=None): # db_manager optional for now
        self.config = config
        self.interface = binance_interface
        self.db_manager = db_manager # May use later for persistence or backfilling gaps
        self.symbols = []
        self.interval = config['kline_interval']
        self.data_fetch_limit = config['data_fetch_limit']
        # Store dataframes, use deque for efficient updates if needed, but DF is often easier
        self.data_store = {} # {symbol: pd.DataFrame}
        # Store the last known factors per symbol for the change threshold filter
        self.last_signal_factors = {} # {symbol: float}
        self.is_initialized = False
        self._websocket_task = None
        self._lock = asyncio.Lock() # Lock for thread-safe access to data_store

        # Determine required lookback based on strategy parameters
        strat_params = config.get('strategy_v2', {})
        self.required_lookback = max(
            strat_params.get('lookback_long', 240),
            strat_params.get('funding_roll', 144) + 5, # Add buffer
            strat_params.get('mr_roll', 96) + 5,
            strat_params.get('flow_roll', 120) + 5,
            strat_params.get('perf_roll', 192) + 5,
            72, 168, 48 # Other rolling windows used
        ) + 10 # General buffer

        logger.info(f"DataHandler initialized. Required lookback: {self.required_lookback} periods.")


    async def initialize_universe(self):
         """ Determines the list of symbols to trade based on config. """
         universe_config = self.config['universe']
         if universe_config['type'] == 'all':
             logger.info("Universe type 'all' selected. Fetching all USDT perpetual futures symbols...")
             all_symbols = await self.interface.get_usdt_futures_symbols(only_perpetual=True)
             if not all_symbols:
                 logger.error("Failed to fetch symbols from Binance. Cannot initialize universe.")
                 return False
             exclude_list = set(universe_config.get('exclude_symbols', []))
             self.symbols = [s for s in all_symbols if s not in exclude_list]
             logger.info(f"Trading universe set to {len(self.symbols)} symbols (excluding {len(exclude_list)}).")
         elif universe_config['type'] == 'list':
             self.symbols = universe_config.get('symbol_list', [])
             if not self.symbols:
                 logger.error("Universe type 'list' selected, but 'symbol_list' is empty in config.")
                 return False
             logger.info(f"Trading universe set from list: {self.symbols}")
         else:
             logger.error(f"Invalid universe type '{universe_config['type']}' in config. Use 'all' or 'list'.")
             return False

         if not self.symbols:
             logger.error("No symbols defined for trading. Halting initialization.")
             return False

         return True


    async def initialize_data(self):
        """Fetches initial historical data for all symbols."""
        if not await self.initialize_universe():
            return False

        logger.info(f"Initializing historical data for {len(self.symbols)} symbols...")
        tasks = [self.interface.fetch_initial_data(symbol, self.interval, self.required_lookback + 100) # Fetch slightly more
                 for symbol in self.symbols]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        successful_symbols = []
        async with self._lock: # Lock when modifying shared state
            for symbol, result in zip(self.symbols, results):
                if isinstance(result, pd.DataFrame) and not result.empty:
                    if len(result) >= self.required_lookback:
                        # Ensure data is sorted by timestamp ascending
                        result.sort_index(inplace=True)
                        # Keep only required lookback + buffer to avoid excessive memory use
                        self.data_store[symbol] = result.tail(self.required_lookback + 5) # Keep a small buffer
                        successful_symbols.append(symbol)
                        logger.debug(f"Initial data loaded for {symbol}. Shape: {self.data_store[symbol].shape}")
                    else:
                         logger.warning(f"Insufficient historical data loaded for {symbol}. Need {self.required_lookback}, got {len(result)}. Excluding symbol.")
                elif isinstance(result, Exception):
                    logger.error(f"Error fetching initial data for {symbol}: {result}")
                else:
                    logger.warning(f"No data returned for {symbol}. Excluding symbol.")

            # Update self.symbols to only include those successfully loaded
            self.symbols = successful_symbols
            if not self.symbols:
                logger.error("Failed to load sufficient initial data for any symbol. Cannot proceed.")
                self.is_initialized = False
                return False

            logger.info(f"Initial data loading complete. Successfully loaded for {len(self.symbols)} symbols.")
            self.is_initialized = True
            return True

    async def start_websockets(self):
        """Starts the Binance websockets for kline data."""
        if not self.is_initialized:
            logger.error("Data handler not initialized. Cannot start websockets.")
            return

        if not self.symbols:
             logger.warning("No symbols to subscribe to for websockets.")
             return

        if self._websocket_task and not self._websocket_task.done():
             logger.warning("Websocket task already running.")
             return

        logger.info("Starting Binance Kline Websockets...")
        self._websocket_task = asyncio.create_task(
            self.interface.start_kline_websockets(self.symbols, self.interval, self._process_kline_message)
        )
        # Add error handling for task creation if needed

    async def stop_websockets(self):
         """Stops the websocket task."""
         if self._websocket_task and not self._websocket_task.done():
             logger.info("Stopping websocket task...")
             self._websocket_task.cancel()
             try:
                 await self._websocket_task
             except asyncio.CancelledError:
                 logger.info("Websocket task successfully cancelled.")
             except Exception as e:
                  logger.error(f"Error during websocket task cancellation: {e}")
         self._websocket_task = None


    async def _process_kline_message(self, msg):
        """Callback function to process incoming kline data from websocket."""
        # Expected msg format (data part):
        # {'e': 'kline', 'E': 1678886400000, 's': 'BTCUSDT', 'k': {
        # 't': 1678886340000, 'T': 1678886399999, 's': 'BTCUSDT', 'i': '1m',
        # 'f': 100, 'L': 200, 'o': '24000.0', 'c': '24010.5', 'h': '24020.0', 'l': '23990.0',
        # 'v': '1000.5', 'n': 500, 'x': False, 'q': '24015000.0',
        # 'V': '500.0', 'Q': '12010000.0', 'B': '0'}}
        try:
            if msg.get('e') != 'kline':
                # logger.debug(f"Received non-kline message: {msg}")
                return

            kline_data = msg.get('k')
            if not kline_data:
                logger.warning(f"Kline data ('k') missing in message: {msg}")
                return

            symbol = kline_data.get('s')
            if not symbol or symbol not in self.symbols:
                # logger.debug(f"Received kline for non-tracked symbol {symbol}")
                return

            is_closed = kline_data.get('x', False) # Check if kline is closed
            open_time_ms = kline_data.get('t')

            if open_time_ms is None:
                 logger.warning(f"Kline open time ('t') missing for {symbol}: {kline_data}")
                 return

            timestamp = pd.to_datetime(open_time_ms, unit='ms')

            # Create a new row (Series) with the kline data
            new_data = {
                DataLabels.OPEN.value: float(kline_data['o']),
                DataLabels.HIGH.value: float(kline_data['h']),
                DataLabels.LOW.value: float(kline_data['l']),
                DataLabels.CLOSE.value: float(kline_data['c']),
                DataLabels.VOLUME.value: float(kline_data['v']),
                DataLabels.TAKER_BUY_VOLUME.value: float(kline_data['V']), # Taker buy base asset volume
                 # Funding Rate and Dollar Volume Delta need separate updates or calculations
                 # Initialize them if missing in the Series
                 DataLabels.FUNDING.value: np.nan,
                 DataLabels.DOLLAR_VOLUME_DELTA.value: np.nan,
            }
            new_row = pd.Series(new_data, name=timestamp)

            async with self._lock: # Acquire lock to update shared data store
                if symbol in self.data_store:
                    df = self.data_store[symbol]

                    # --- Update or Append Logic ---
                    if not df.empty and timestamp == df.index[-1]:
                        # Update the last row (candle is still open)
                        df.iloc[-1] = new_row
                        # logger.debug(f"Updated open candle for {symbol} at {timestamp}")
                    elif timestamp > df.index[-1]:
                        # Append the new closed candle
                        # Before appending, fill missing funding/delta for the PREVIOUS candle
                        if pd.isna(df.iloc[-1][DataLabels.FUNDING.value]):
                             df.iloc[-1, df.columns.get_loc(DataLabels.FUNDING.value)] = df.iloc[-2][DataLabels.FUNDING.value] if len(df) > 1 else 0.0 # Carry forward last known funding

                        # Calculate dollar delta for the COMPLETED previous candle before appending new one
                        # Note: This uses the kline's taker buy vol, which is an approximation.
                        # Real delta needs agg trades.
                        prev_row_for_delta = df.iloc[-1]
                        delta = calculate_dollar_volume_delta(prev_row_for_delta.to_frame().T).iloc[0] # Calculate for the single row
                        df.iloc[-1, df.columns.get_loc(DataLabels.DOLLAR_VOLUME_DELTA.value)] = delta


                        # Now append the new row (it will have NaN funding/delta initially)
                        # Use pd.concat instead of append for newer pandas versions
                        new_row_df = new_row.to_frame().T
                        # Ensure new_row_df has the same columns in the same order
                        new_row_df = new_row_df.reindex(columns=df.columns)
                        df = pd.concat([df, new_row_df])

                        # Carry forward funding rate to the newly added row immediately
                        df.loc[timestamp, DataLabels.FUNDING.value] = df.loc[df.index[-2], DataLabels.FUNDING.value] if len(df) > 1 else 0.0


                        # Trim the DataFrame to maintain size
                        if len(df) > self.required_lookback + 10: # Keep buffer
                            df = df.iloc[-(self.required_lookback + 10):]

                        self.data_store[symbol] = df
                        # logger.debug(f"Appended new candle for {symbol} at {timestamp}. IsClosed: {is_closed}. New shape: {df.shape}")

                    elif timestamp < df.index[-1]:
                         # Received an older candle? Should not happen often with websockets
                         logger.warning(f"Received out-of-order kline for {symbol}. TS: {timestamp}, Last: {df.index[-1]}")
                         # Potentially insert/ignore based on logic
                    # else: # timestamp == df.index[-1] already handled

                    # Fill NaN funding rates after update/append (forward fill)
                    # This might be redundant if handled above, but safe to keep
                    # self.data_store[symbol][DataLabels.FUNDING.value].ffill(inplace=True)
                    # self.data_store[symbol][DataLabels.FUNDING.value].fillna(0, inplace=True) # Fill any remaining at start


                else:
                    logger.warning(f"Received kline for {symbol}, but no initial data loaded.")

        except ValueError as e:
            logger.error(f"ValueError processing kline for {symbol}: {e}. Data: {kline_data}")
        except KeyError as e:
            logger.error(f"KeyError processing kline message: {e}. Message: {msg}")
        except Exception as e:
            logger.exception(f"Unexpected error processing kline message for {symbol}: {e}") # Log traceback


    async def get_latest_data(self, symbol):
        """Returns the latest DataFrame for a symbol."""
        async with self._lock: # Ensure read is safe
             if symbol in self.data_store:
                 # Return a copy to prevent modification outside the handler
                 return self.data_store[symbol].copy()
             else:
                 logger.warning(f"No data available for symbol {symbol} in get_latest_data")
                 return pd.DataFrame()

    async def get_last_close_price(self, symbol):
         """ Gets the last close price for a symbol efficiently. """
         async with self._lock:
             if symbol in self.data_store and not self.data_store[symbol].empty:
                 try:
                     return self.data_store[symbol][DataLabels.CLOSE.value].iloc[-1]
                 except IndexError:
                      logger.warning(f"Index error getting last close for {symbol}. Data possibly empty.")
                      return None
                 except Exception as e:
                      logger.error(f"Error getting last close price for {symbol}: {e}")
                      return None
             else:
                 # Fallback to API if necessary? Or handle missing data upstream.
                 # logger.warning(f"No data in store for {symbol} to get last close price.")
                 # As a fallback, try getting ticker price
                 ticker_info = await self.interface.get_ticker(symbol)
                 return ticker_info['price'] if ticker_info else None


    # --- Methods for turnover reduction filter state ---
    async def get_last_signal_factor(self, symbol):
        """ Gets the last known final factor for a symbol. """
        async with self._lock:
            return self.last_signal_factors.get(symbol, 0.0) # Default to 0 if not found

    async def set_last_signal_factor(self, symbol, factor):
        """ Sets the last known final factor for a symbol. """
        async with self._lock:
            self.last_signal_factors[symbol] = factor

    async def update_funding_rates_periodically(self):
        """ Periodically fetches latest funding rates to ensure data freshness. """
        # This should run in a separate task scheduled in main.py
        while True:
            await asyncio.sleep(3600 * 4) # Update every 4 hours, e.g.
            logger.info("Periodically updating funding rates...")
            if not self.symbols:
                 continue

            tasks = {symbol: self.interface.get_funding_rate_history(symbol, limit=5) for symbol in self.symbols} # Fetch recent rates
            results = await asyncio.gather(*tasks.values(), return_exceptions=True)

            results_dict = dict(zip(tasks.keys(), results))

            async with self._lock:
                for symbol, funding_df in results_dict.items():
                    if isinstance(funding_df, pd.DataFrame) and not funding_df.empty:
                        if symbol in self.data_store:
                             df = self.data_store[symbol]
                             # Update existing dataframe with new funding rates
                             # Need careful merging/updating based on timestamp
                             # Option 1: Re-apply ffill based on latest available data
                             funding_df.index = pd.to_datetime(funding_df.index)
                             # Combine, keeping the latest funding rate for each timestamp
                             # This needs a more robust merge/update strategy if historical accuracy is paramount
                             # For simplicity, let's just ensure the latest value is propogated forward
                             latest_rate = funding_df['funding_rate'].iloc[-1]
                             latest_ts = funding_df.index[-1]
                             # Find rows in df after the latest funding timestamp and update if NaN or older
                             mask = df.index >= latest_ts
                             # A simple forward fill might be sufficient given the websocket updates carry forward
                             df[DataLabels.FUNDING.value].ffill(inplace=True)
                             # Ensure the very last row gets the absolute latest rate if its timestamp matches/exceeds
                             if df.index[-1] >= latest_ts:
                                  df.iloc[-1, df.columns.get_loc(DataLabels.FUNDING.value)] = latest_rate

                             logger.debug(f"Updated funding rate for {symbol} based on periodic check.")
                        else:
                             logger.warning(f"Symbol {symbol} found in funding update but not in main data store.")
                    elif isinstance(funding_df, Exception):
                         logger.error(f"Error updating funding rate for {symbol}: {funding_df}")
                    # else: No new funding data fetched

            logger.info("Periodic funding rate update finished.")
```

---

**9. `core/position_manager.py`**

```python
import asyncio
import pandas as pd
from database.db_manager import DatabaseManager
from interfaces.binance_interface import BinanceInterface
from utils.logger import setup_logger
from decimal import Decimal, ROUND_HALF_UP

logger = setup_logger(__name__)

class PositionManager:
    def __init__(self, config, db_manager: DatabaseManager, binance_interface: BinanceInterface):
        self.config = config
        self.db = db_manager
        self.interface = binance_interface
        self.trading_mode = config['trading_mode']
        # In-memory store for current positions {symbol: {mode: str, quantity: Decimal, entry_price: Decimal, leverage: int, unrealized_pnl: Decimal, mark_price: Decimal, initial_margin: Decimal}}
        self.positions = {}
        self._lock = asyncio.Lock() # Lock for accessing/modifying positions

    async def initialize_positions(self):
        """Loads positions from DB or Binance API on startup."""
        logger.info("Initializing Position Manager...")
        async with self._lock:
            self.positions = {} # Clear existing memory state
            if self.trading_mode in ['live', 'testnet']:
                logger.info(f"Loading positions from Binance API for {self.trading_mode} mode...")
                try:
                    api_positions = await self.interface.get_position_information()
                    if api_positions:
                        loaded_count = 0
                        for pos in api_positions:
                            symbol = pos.get('symbol')
                            quantity = Decimal(pos.get('positionAmt', '0'))
                            if symbol and quantity != Decimal(0): # Only load active positions
                                entry_price = Decimal(pos.get('entryPrice', '0'))
                                leverage = int(pos.get('leverage', '0'))
                                unrealized_pnl = Decimal(pos.get('unRealizedProfit', '0'))
                                mark_price = Decimal(pos.get('markPrice', '0'))
                                # Calculate approximate initial margin (Needs careful check against Binance definition)
                                initial_margin = (quantity.copy_abs() * entry_price) / Decimal(leverage) if leverage > 0 and entry_price > 0 else Decimal(0)

                                self.positions[symbol] = {
                                    'mode': self.trading_mode,
                                    'quantity': quantity,
                                    'entry_price': entry_price,
                                    'leverage': leverage,
                                    'unrealized_pnl': unrealized_pnl,
                                    'mark_price': mark_price,
                                    'initial_margin': initial_margin
                                }
                                loaded_count += 1
                                # Also update the database to reflect API state
                                await self.db.update_position(self.positions[symbol] | {'symbol': symbol}) # Merge dicts
                        logger.info(f"Loaded {loaded_count} active positions from Binance API.")
                    else:
                         logger.info("No active positions found on Binance API.")

                except Exception as e:
                    logger.error(f"Failed to load positions from Binance API: {e}. Falling back to DB.")
                    # Fallback to DB load if API fails
                    await self._load_from_db()

            elif self.trading_mode == 'paper':
                logger.info("Loading positions from Database for paper mode...")
                await self._load_from_db()
            else:
                 logger.error(f"Invalid trading mode for position initialization: {self.trading_mode}")

        logger.info("Position Manager initialization complete.")
        logger.info(f"Current positions: {await self.get_all_positions()}") # Log initial state


    async def _load_from_db(self):
        """Helper to load positions from the database."""
        db_positions = await self.db.get_all_positions(self.trading_mode)
        if db_positions:
            for pos in db_positions:
                symbol = pos['symbol']
                # Ensure values are converted to Decimal where appropriate
                self.positions[symbol] = {
                    'mode': pos['mode'],
                    'quantity': Decimal(str(pos['quantity'])), # Convert DB Decimal/float string
                    'entry_price': Decimal(str(pos['entry_price'])),
                    'leverage': int(pos['leverage']),
                    'unrealized_pnl': Decimal(str(pos['unrealized_pnl'])) if pos['unrealized_pnl'] is not None else Decimal(0),
                    'mark_price': Decimal(str(pos['mark_price'])) if pos['mark_price'] is not None else Decimal(0),
                     'initial_margin': Decimal(str(pos['initial_margin'])) if pos['initial_margin'] is not None else Decimal(0)
                }
            logger.info(f"Loaded {len(self.positions)} positions from database for mode {self.trading_mode}.")
        else:
            logger.info(f"No active positions found in database for mode {self.trading_mode}.")


    async def update_position_from_trade(self, trade_info):
        """
        Updates position based on a filled trade. Calculates new entry price and realized PnL.

        Args:
            trade_info: Dictionary containing trade details like
                        symbol, side, quantity, price, commission, mode, etc.

        Returns:
             The updated position dictionary or None if error.
             Also returns the calculated realized PnL for this trade.
        """
        async with self._lock:
            symbol = trade_info['symbol']
            mode = trade_info['mode']
            trade_side = trade_info['side'].upper() # BUY or SELL
            trade_qty = Decimal(str(trade_info['quantity'])) # Amount of base asset traded
            trade_price = Decimal(str(trade_info['price']))
            # Commission is deducted from quote asset (usually USDT) or base asset
            # We need total PnL, so commission impact needs calculation later in PnL module or here
            # commission = Decimal(str(trade_info.get('commission', '0')))
            # commission_asset = trade_info.get('commission_asset')

            realized_pnl = Decimal(0)
            current_pos = self.positions.get(symbol)

            if current_pos:
                current_qty = current_pos['quantity']
                current_entry = current_pos['entry_price']
                leverage = current_pos['leverage']

                new_qty = current_qty
                new_entry = current_entry

                # Determine if the trade closes, opens, increases, or reduces the position
                if (trade_side == 'BUY' and current_qty > 0) or \
                   (trade_side == 'SELL' and current_qty < 0): # Increasing position
                    # Calculate new average entry price
                    current_value = current_qty.copy_abs() * current_entry
                    trade_value = trade_qty * trade_price
                    new_qty = current_qty + (trade_qty if trade_side == 'BUY' else -trade_qty)
                    new_entry = (current_value + trade_value) / new_qty.copy_abs() if new_qty != 0 else Decimal(0)
                    logger.debug(f"{symbol}: Increased position. Qty: {current_qty}->{new_qty}, Entry: {current_entry}->{new_entry}")

                elif (trade_side == 'SELL' and current_qty > 0) or \
                     (trade_side == 'BUY' and current_qty < 0): # Reducing or Closing position

                    if trade_qty >= current_qty.copy_abs(): # Closing or Flipping position
                        # Calculate realized PnL for the closed portion (entire current position)
                        if current_qty > 0: # Closing Long
                             realized_pnl = (trade_price - current_entry) * current_qty.copy_abs()
                        else: # Closing Short
                             realized_pnl = (current_entry - trade_price) * current_qty.copy_abs()

                        remaining_trade_qty = trade_qty - current_qty.copy_abs()
                        new_qty = Decimal(0) # Position closed initially
                        new_entry = Decimal(0)

                        if remaining_trade_qty > 0: # Flipped position (opened opposite)
                            new_qty = -remaining_trade_qty if trade_side == 'SELL' else remaining_trade_qty
                            new_entry = trade_price # Entry price is the flip price
                            logger.debug(f"{symbol}: Flipped position. Closed: {current_qty}@{current_entry}. Opened: {new_qty}@{new_entry}. R PnL: {realized_pnl:.4f}")
                        else:
                             logger.debug(f"{symbol}: Closed position. Qty: {current_qty}->{new_qty}. R PnL: {realized_pnl:.4f}")


                    else: # Reducing position size
                         # Calculate realized PnL for the reduced portion
                         if current_qty > 0: # Reducing Long
                             realized_pnl = (trade_price - current_entry) * trade_qty
                         else: # Reducing Short
                             realized_pnl = (current_entry - trade_price) * trade_qty

                         new_qty = current_qty - trade_qty if trade_side == 'SELL' else current_qty + trade_qty
                         new_entry = current_entry # Entry price doesn't change when only reducing size
                         logger.debug(f"{symbol}: Reduced position. Qty: {current_qty}->{new_qty}. Entry: {new_entry}. R PnL: {realized_pnl:.4f}")

                else:
                     logger.error(f"Inconsistent trade side/position state for {symbol}. CurrentQty: {current_qty}, TradeSide: {trade_side}")
                     return None, Decimal(0) # Error case

                # Update position in memory
                current_pos['quantity'] = new_qty
                current_pos['entry_price'] = new_entry.quantize(Decimal('0.00000001'), rounding=ROUND_HALF_UP) if new_qty != 0 else Decimal(0)
                current_pos['mark_price'] = trade_price # Update mark price to last trade price initially
                # Recalculate initial margin (approximation)
                current_pos['initial_margin'] = (new_qty.copy_abs() * current_pos['entry_price']) / Decimal(leverage) if leverage > 0 and current_pos['entry_price'] > 0 else Decimal(0)

                # If position is closed, remove from dict, otherwise update
                if new_qty == Decimal(0):
                     # Update DB one last time with zero quantity before removing
                     await self.db.update_position(current_pos | {'symbol': symbol, 'unrealized_pnl': 0, 'initial_margin': 0}) # Merge dicts
                     del self.positions[symbol]
                     logger.info(f"Position closed for {symbol}")
                else:
                    # Update unrealized PnL based on last trade price (will be updated again by mark price later)
                    current_pos['unrealized_pnl'] = self.calculate_unrealized_pnl_sync(current_pos)
                    await self.db.update_position(current_pos | {'symbol': symbol}) # Merge dicts

                return self.positions.get(symbol), realized_pnl.quantize(Decimal('0.00000001'), rounding=ROUND_HALF_UP)

            else: # No existing position, this is an opening trade
                 new_qty = trade_qty if trade_side == 'BUY' else -trade_qty
                 new_entry = trade_price
                 # Get leverage (should be set before trading)
                 leverage = self.config.get('leverage', 1) # Default leverage if not found? Risky. Assume it's set elsewhere.
                 # TODO: Need a way to fetch/store leverage per symbol if not uniform
                 initial_margin = (trade_qty * trade_price) / Decimal(leverage) if leverage > 0 else Decimal(0)

                 new_pos = {
                    'mode': mode,
                    'quantity': new_qty,
                    'entry_price': new_entry,
                    'leverage': leverage,
                    'unrealized_pnl': Decimal(0), # No PnL yet
                    'mark_price': trade_price, # Initial mark price
                    'initial_margin': initial_margin
                 }
                 self.positions[symbol] = new_pos
                 await self.db.update_position(new_pos | {'symbol': symbol}) # Merge dicts
                 logger.info(f"Opened new position for {symbol}: Qty: {new_qty}, Entry: {new_entry}")
                 return new_pos, Decimal(0) # No realized PnL on opening trade

    def calculate_unrealized_pnl_sync(self, position_data):
        """Synchronously calculates unrealized PnL for a single position dict."""
        quantity = position_data['quantity']
        entry_price = position_data['entry_price']
        mark_price = position_data['mark_price']

        if quantity == Decimal(0) or mark_price is None or mark_price <= Decimal(0):
             return Decimal(0)

        if quantity > 0: # Long position
            pnl = (mark_price - entry_price) * quantity
        else: # Short position
            pnl = (entry_price - mark_price) * quantity.copy_abs()

        return pnl.quantize(Decimal('0.00000001'), rounding=ROUND_HALF_UP)


    async def update_mark_prices(self, mark_prices: dict):
        """Updates mark prices and unrealized PnL for all managed positions."""
        async with self._lock:
            if not self.positions: # No positions to update
                 return

            symbols_to_update_db = []
            total_unrealized_pnl = Decimal(0)

            for symbol, price_info in mark_prices.items():
                if symbol in self.positions:
                    mark_price = Decimal(str(price_info['price']))
                    pos = self.positions[symbol]
                    pos['mark_price'] = mark_price
                    pos['unrealized_pnl'] = self.calculate_unrealized_pnl_sync(pos)
                    total_unrealized_pnl += pos['unrealized_pnl']
                    symbols_to_update_db.append(pos | {'symbol': symbol}) # Merge symbol for DB update

            # Batch update DB if possible, or update one by one
            if symbols_to_update_db:
                # logger.debug(f"Updating mark prices/PnL for {len(symbols_to_update_db)} positions.")
                # This could be optimized with a bulk update query if the DB manager supports it
                update_tasks = [self.db.update_position(pos_data) for pos_data in symbols_to_update_db]
                await asyncio.gather(*update_tasks)

            return total_unrealized_pnl # Return the total PnL for this update cycle


    async def get_position(self, symbol):
        """Gets a specific position's details."""
        async with self._lock:
            # Return a copy to prevent external modification
            return self.positions.get(symbol, None)

    async def get_all_positions(self):
        """Gets details for all current positions."""
        async with self._lock:
            # Return a deep copy to prevent external modification
            return {symbol: pos.copy() for symbol, pos in self.positions.items()}

    async def get_total_positions_value(self):
        """ Calculates the total absolute value of all open positions. """
        async with self._lock:
            total_value = Decimal(0)
            for symbol, pos in self.positions.items():
                mark_price = pos.get('mark_price')
                quantity = pos.get('quantity')
                if mark_price and quantity is not None and mark_price > 0:
                    total_value += quantity.copy_abs() * mark_price
            return total_value

    async def get_total_initial_margin(self):
         """ Calculates the sum of initial margin for all positions. """
         async with self._lock:
             total_margin = Decimal(0)
             for symbol, pos in self.positions.items():
                 margin = pos.get('initial_margin', Decimal(0))
                 total_margin += margin
             return total_margin

```

---

**10. `core/pnl_calculator.py`** (Simplified version)

```python
import asyncio
from decimal import Decimal, ROUND_HALF_UP
from database.db_manager import DatabaseManager
from core.position_manager import PositionManager # Use for getting unrealized PnL
from utils.logger import setup_logger
from datetime import datetime

logger = setup_logger(__name__)

class PnLCalculator:
    def __init__(self, config, db_manager: DatabaseManager, position_manager: PositionManager):
        self.config = config
        self.db = db_manager
        self.position_manager = position_manager # Needed to get total unrealized PnL
        self.trading_mode = config['trading_mode']
        self.initial_capital = Decimal(str(config.get('initial_capital_usd', 0))) # For paper/testnet

        # In-memory state for cumulative PnL and fees (loaded on init)
        self.cumulative_realized_pnl = Decimal(0)
        self.cumulative_fees = Decimal(0)
        self.cumulative_trades = 0
        self._lock = asyncio.Lock()

    async def initialize_pnl(self):
        """Load last performance snapshot to initialize cumulative values."""
        logger.info("Initializing PnL Calculator...")
        async with self._lock:
            last_snapshot = await self.db.get_latest_performance(self.trading_mode)
            if last_snapshot:
                self.cumulative_realized_pnl = Decimal(str(last_snapshot.get('realized_pnl_cumulative', 0)))
                self.cumulative_fees = Decimal(str(last_snapshot.get('total_fees_cumulative', 0)))
                self.cumulative_trades = int(last_snapshot.get('trade_count_cumulative', 0))
                logger.info(f"Loaded PnL state from DB: Realized={self.cumulative_realized_pnl:.2f}, Fees={self.cumulative_fees:.4f}, Trades={self.cumulative_trades}")
            else:
                logger.info("No previous performance snapshot found. Starting PnL from zero.")
                self.cumulative_realized_pnl = Decimal(0)
                self.cumulative_fees = Decimal(0)
                self.cumulative_trades = 0
        logger.info("PnL Calculator initialized.")


    async def record_trade_pnl(self, trade_info, realized_pnl):
        """
        Updates cumulative PnL and fees based on a completed trade.
        Assumes realized_pnl is passed from PositionManager.update_position_from_trade.
        """
        async with self._lock:
            commission = Decimal(str(trade_info.get('commission', '0')))
            commission_asset = trade_info.get('commission_asset')
            quote_qty = Decimal(str(trade_info.get('quote_quantity', '0'))) # Price * Qty
            price = Decimal(str(trade_info.get('price', '0')))

            # --- Fee Calculation ---
            # Simplified: Assume commission is always in USDT (quote asset) for futures
            # If commission is in base asset, need conversion using trade price
            fee_in_usdt = Decimal(0)
            if commission_asset and commission_asset == 'USDT': # Or other quote assets like BUSD
                 fee_in_usdt = commission
            elif commission_asset and commission_asset != trade_info['symbol'].replace('USDT', ''): # Check if commission is in base asset like BTC for BTCUSDT
                 # This conversion might be slightly off depending on exact fee structure
                 # Only do this if commission asset is NOT the quote asset (USDT)
                 if price > 0:
                      fee_in_usdt = commission * price # Convert base asset commission to quote value
                 else:
                      logger.warning(f"Cannot convert commission asset {commission_asset} for {trade_info['symbol']} due to zero price.")
            elif commission > 0: # If asset is not specified but commission exists, assume quote asset fee
                 logger.warning(f"Commission asset not specified for trade {trade_info.get('order_id', '')}. Assuming USDT fee.")
                 fee_in_usdt = commission

            self.cumulative_realized_pnl += realized_pnl
            self.cumulative_fees += fee_in_usdt
            self.cumulative_trades += 1

            logger.debug(f"Trade Recorded PnL: R PnL={realized_pnl:.4f}, Fee={fee_in_usdt:.4f}. Cumulative: R PnL={self.cumulative_realized_pnl:.2f}, Fees={self.cumulative_fees:.4f}, Trades={self.cumulative_trades}")

    async def calculate_and_store_performance_snapshot(self, current_equity=None):
        """
        Calculates a snapshot of the portfolio performance and stores it in the DB.
        Requires current total equity. For paper/testnet, this is calculated.
        For live, it should ideally come from the exchange balance API.
        """
        async with self._lock:
            timestamp = datetime.now(timezone.utc).replace(tzinfo=None) # Use UTC, remove tzinfo for DB

            # 1. Get Total Unrealized PnL from Position Manager
            # We need to trigger mark price updates *before* calling this ideally
            all_positions = await self.position_manager.get_all_positions()
            total_unrealized_pnl = sum(p.get('unrealized_pnl', Decimal(0)) for p in all_positions.values())

            # 2. Determine Current Equity
            if self.trading_mode == 'paper':
                 # Calculate equity based on initial capital and cumulative P&L
                 current_equity = self.initial_capital + self.cumulative_realized_pnl + total_unrealized_pnl - self.cumulative_fees
            elif current_equity is None and self.trading_mode in ['live', 'testnet']:
                 # Fetch from balance API if not provided (needs implementation in interface/engine)
                 logger.warning("Current equity not provided for performance snapshot in live/testnet mode. Calculation might be inaccurate.")
                 # As a fallback, use paper calculation (less accurate for live)
                 # TODO: Fetch actual balance from BinanceInterface
                 # balance_info = await self.position_manager.interface.get_account_balance()
                 # usdt_balance = next((item for item in balance_info if item['asset'] == 'USDT'), None)
                 # if usdt_balance:
                 #     current_equity = Decimal(usdt_balance['balance']) + total_unrealized_pnl # Simplified equity calculation
                 # else:
                 #      logger.error("Could not fetch USDT balance for equity calculation.")
                 #      current_equity = self.initial_capital + self.cumulative_realized_pnl + total_unrealized_pnl - self.cumulative_fees # Fallback
                 current_equity = self.initial_capital + self.cumulative_realized_pnl + total_unrealized_pnl - self.cumulative_fees # Temp fallback

            elif self.trading_mode not in ['paper', 'live', 'testnet']:
                 logger.error(f"Invalid trading mode {self.trading_mode} for snapshot.")
                 return

            # 3. Prepare Snapshot Data
            snapshot_data = {
                'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'), # Format for DB
                'mode': self.trading_mode,
                'total_equity': current_equity.quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP),
                'realized_pnl_cumulative': self.cumulative_realized_pnl.quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP),
                'unrealized_pnl': total_unrealized_pnl.quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP),
                'total_pnl_cumulative': (self.cumulative_realized_pnl + total_unrealized_pnl).quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP),
                'total_fees_cumulative': self.cumulative_fees.quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP),
                'trade_count_cumulative': self.cumulative_trades
            }

            # 4. Store in DB
            success = await self.db.insert_performance_snapshot(snapshot_data)
            if success:
                # logger.debug(f"Performance snapshot stored successfully at {snapshot_data['timestamp']}. Equity: {snapshot_data['total_equity']:.2f}")
                pass
            else:
                logger.error(f"Failed to store performance snapshot at {snapshot_data['timestamp']}")

    async def get_current_equity(self):
        """ Returns the estimated current equity. """
        # This provides an estimate based on last known PnL state.
        # A more accurate version would involve real-time balance fetching for live modes.
        async with self._lock:
             all_positions = await self.position_manager.get_all_positions()
             total_unrealized_pnl = sum(p.get('unrealized_pnl', Decimal(0)) for p in all_positions.values())
             equity = self.initial_capital + self.cumulative_realized_pnl + total_unrealized_pnl - self.cumulative_fees
             return equity

```

---

**11. `execution/executor.py`**

```python
import asyncio
import time
from decimal import Decimal, ROUND_HALF_UP
from interfaces.binance_interface import BinanceInterface
from database.db_manager import DatabaseManager
from core.position_manager import PositionManager
from core.pnl_calculator import PnLCalculator
from utils.logger import setup_logger
from datetime import datetime, timezone

logger = setup_logger(__name__)

# Define constants for typical Binance Futures fee (adjust if needed)
MAKER_FEE = Decimal('0.0002') # 0.02%
TAKER_FEE = Decimal('0.0004') # 0.04%

class Executor:
    def __init__(self, config, interface: BinanceInterface, db_manager: DatabaseManager,
                 position_manager: PositionManager, pnl_calculator: PnLCalculator, data_handler): # Added data_handler
        self.config = config
        self.interface = interface
        self.db = db_manager
        self.position_manager = position_manager
        self.pnl_calculator = pnl_calculator
        self.data_handler = data_handler # Needed for last prices in paper mode
        self.trading_mode = config['trading_mode']
        self.leverage = Decimal(str(config.get('leverage', 1))) # Global leverage
        self.target_volatility = Decimal(str(config.get('target_signal_volatility', 0.2)))
        self.max_pos_risk_pct = Decimal(str(config.get('max_position_risk_per_trade', 0.01))) # Not fully implemented yet
        self.max_total_risk_pct = Decimal(str(config.get('max_total_risk_exposure', 0.10))) # Not fully implemented yet


    def _get_paper_trade_commission(self, symbol, quantity, price):
        """Calculates simulated commission for paper trading."""
        quote_quantity = quantity.copy_abs() * price
        # Assume TAKER fee for market orders in paper trading
        commission = quote_quantity * TAKER_FEE
        commission_asset = symbol.replace(symbol.replace('USDT',''), '') # Assume quote asset is USDT
        return commission, commission_asset

    async def calculate_target_position_usd(self, symbol: str, final_factor: float, current_equity: Decimal) -> Decimal:
        """
        Calculates the target position size in USD based on the factor and equity.
        Factor is assumed to be scaled to target volatility already.
        """
        if current_equity <= 0:
            logger.warning(f"Cannot calculate target position for {symbol}, equity is zero or negative.")
            return Decimal(0)

        # The final_factor is already scaled by target volatility (e.g., 0.2)
        # A factor of 0.2 means target position = 1 * leverage * equity * 0.2
        # A factor of -0.1 means target position = -1 * leverage * equity * 0.1
        # The factor directly represents the fraction of leveraged equity to allocate.

        # Simple Allocation: target_usd = factor * leverage * equity
        # Note: factor range isn't strictly [-1, 1], can exceed due to vol scaling. Clip it?
        # Clipping factor to [-1, 1] might be safer to prevent extreme leverage usage.
        capped_factor = Decimal(str(max(min(final_factor, 1.0), -1.0)))

        target_position_usd = capped_factor * self.leverage * current_equity

        # --- Apply Risk Limits (Basic Implementation) ---
        # TODO: Implement max_position_risk_per_trade (needs stop-loss logic)
        # TODO: Implement max_total_risk_exposure (needs tracking total margin/risk)

        # Check against max total risk (approximated by total margin usage)
        # current_total_margin = await self.position_manager.get_total_initial_margin()
        # max_allowable_margin = current_equity * self.max_total_risk_pct * self.leverage # Simplified check
        # potential_new_margin = (target_position_usd.copy_abs() / self.leverage) # Margin for this target
        # if current_total_margin + potential_new_margin > max_allowable_margin:
        #     logger.warning(f"{symbol}: Target position {target_position_usd:.2f} USD exceeds max total risk exposure. Reducing.")
        #     # Scale down target_position_usd based on available risk budget
        #     # This needs careful calculation based on available margin/risk capacity

        # logger.debug(f"{symbol}: Factor={final_factor:.4f}, Equity={current_equity:.2f}, Target Position={target_position_usd:.2f} USD")
        return target_position_usd.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)


    async def execute_trade(self, symbol: str, target_position_usd: Decimal, current_position_data: dict | None):
        """
        Compares target position with current and executes market orders if needed.

        Args:
            symbol: The symbol to trade.
            target_position_usd: The desired position value in USD.
            current_position_data: The current position details from PositionManager, or None.
        """
        # 1. Get Current Position Size and Mark Price
        current_qty = Decimal(0)
        if current_position_data:
            current_qty = current_position_data['quantity']

        # Get the latest price for conversion and paper trading fills
        last_price = await self.data_handler.get_last_close_price(symbol)
        if last_price is None:
            logger.error(f"Cannot execute trade for {symbol}, failed to get current price.")
            return
        mark_price = Decimal(str(last_price))

        # 2. Calculate Current Position Value
        current_position_usd = current_qty * mark_price

        # 3. Calculate Difference and Required Trade Size (USD)
        trade_value_usd = target_position_usd - current_position_usd

        # 4. Determine Trade Side and Quantity (Base Asset)
        if abs(trade_value_usd) < mark_price * Decimal('0.00001'): # Ignore tiny trades (dust)
            # logger.debug(f"{symbol}: No significant trade needed. Target: {target_position_usd:.2f}, Current: {current_position_usd:.2f} USD")
            return

        trade_side = "BUY" if trade_value_usd > 0 else "SELL"
        trade_qty_base = (trade_value_usd / mark_price).copy_abs()

        # --- TODO: Add Order Size Constraints ---
        # - Minimum order size (fetch from exchange info)
        # - Maximum order size
        # - Step size / precision (fetch from exchange info)
        # Need to fetch exchange info and apply rounding/clipping here

        if trade_qty_base <= Decimal(0):
             logger.warning(f"{symbol}: Calculated trade quantity is zero or negative ({trade_qty_base}). Skipping trade.")
             return

        # 5. Execute Order (Paper or Live/Testnet)
        order_result = None
        client_order_id = f"{self.trading_mode}_{symbol}_{int(time.time()*1000)}"

        logger.info(f"{symbol}: Target={target_position_usd:.2f} USD, Current={current_position_usd:.2f} USD. Action: {trade_side} {trade_qty_base:.6f} @ ~{mark_price:.4f}")

        try:
            order_result = await self.interface.place_order(
                symbol=symbol,
                side=trade_side,
                quantity=float(trade_qty_base), # Binance API often expects float here, confirm library usage
                order_type='MARKET',
                position_side='BOTH', # Assuming one-way mode
                new_client_order_id=client_order_id
            )

        except Exception as e:
            logger.error(f"Error placing order for {symbol}: {e}")
            # Handle failed order placement (e.g., insufficient margin)
            return # Stop processing this trade

        # 6. Process Order Result
        if order_result and order_result.get('status') in ['FILLED', 'PARTIALLY_FILLED'] or order_result.get('is_paper'):
            if order_result.get('is_paper'):
                # Paper trade simulation fill
                filled_qty = trade_qty_base
                avg_price = mark_price # Assume fill at current mark price for paper
                commission, commission_asset = self._get_paper_trade_commission(symbol, filled_qty, avg_price)
                quote_qty = filled_qty * avg_price
                is_maker = False # Market orders are takers
                order_id = order_result.get('orderId')
                trade_time = datetime.now(timezone.utc).replace(tzinfo=None) # Use current time for paper
                status = 'FILLED'
            else:
                 # Live/Testnet trade fill details from Binance response
                 # Need to handle partial fills carefully - this assumes full fill for simplicity
                 status = order_result.get('status')
                 if status == 'FILLED':
                     filled_qty = Decimal(str(order_result.get('executedQty', '0')))
                     avg_price = Decimal(str(order_result.get('avgPrice', '0')))
                     # Fetch fill details if needed for precise commission (requires querying trades)
                     # For simplicity, use avgPrice and assume taker fee
                     quote_qty = Decimal(str(order_result.get('cumQuote', '0'))) # Use cumQuote if available
                     # Infer commission roughly (Binance doesn't return it directly in order result)
                     # Needs call to get order trades or user data stream
                     # Approximate taker commission
                     approx_commission = quote_qty * TAKER_FEE
                     commission = approx_commission # Placeholder
                     commission_asset = symbol.replace(symbol.replace('USDT',''), '') # Assume quote USDT
                     is_maker = False # Market orders are generally takers
                     order_id = order_result.get('orderId')
                     trade_time_ms = order_result.get('updateTime')
                     trade_time = pd.to_datetime(trade_time_ms, unit='ms') if trade_time_ms else datetime.now(timezone.utc).replace(tzinfo=None)

                 else: # PARTIALLY_FILLED or other non-error status
                      logger.warning(f"{symbol}: Order status is {status}. Handling partial fills is not fully implemented. Processing as filled qty.")
                      # Process the executed portion
                      filled_qty = Decimal(str(order_result.get('executedQty', '0')))
                      avg_price = Decimal(str(order_result.get('avgPrice', '0'))) # Avg price might be 0 if not filled yet
                      if filled_qty > 0 and avg_price <= 0:
                           # Try to get price from ticker again if avgPrice is invalid
                           ticker_price = await self.data_handler.get_last_close_price(symbol)
                           avg_price = Decimal(str(ticker_price)) if ticker_price else mark_price # Fallback

                      if filled_qty <= 0:
                           logger.warning(f"{symbol}: Order {status} but filled quantity is zero. Ignoring.")
                           return

                      quote_qty = filled_qty * avg_price # Estimate quote qty
                      approx_commission = quote_qty * TAKER_FEE
                      commission = approx_commission
                      commission_asset = symbol.replace(symbol.replace('USDT',''), '')
                      is_maker = False
                      order_id = order_result.get('orderId')
                      trade_time_ms = order_result.get('updateTime')
                      trade_time = pd.to_datetime(trade_time_ms, unit='ms') if trade_time_ms else datetime.now(timezone.utc).replace(tzinfo=None)

            # Prepare trade data for DB and Position Manager
            trade_data_for_db = {
                'symbol': symbol,
                'mode': self.trading_mode,
                'order_id': order_id,
                'client_order_id': order_result.get('clientOrderId'),
                'side': trade_side,
                'position_side': order_result.get('positionSide', 'BOTH'),
                'order_type': order_result.get('origType', 'MARKET'),
                'price': avg_price.quantize(Decimal('0.00000001')),
                'quantity': filled_qty.quantize(Decimal('0.00000001')),
                'quote_quantity': quote_qty.quantize(Decimal('0.000001')),
                'commission': commission.quantize(Decimal('0.000001')),
                'commission_asset': commission_asset,
                'is_maker': is_maker,
                'trade_time': trade_time,
                'strategy_factor': float(target_position_usd / (await self.pnl_calculator.get_current_equity() * self.leverage)) if await self.pnl_calculator.get_current_equity() * self.leverage != 0 else 0, # Store approximate factor
                # Realized PNL will be added after position update
            }

            # 7. Update Position Manager and Get Realized PnL
            updated_position, realized_pnl = await self.position_manager.update_position_from_trade(trade_data_for_db)

            # 8. Record Trade in DB (with realized PnL) and Update PnL Calculator state
            if updated_position is not None: # Check if position update was successful
                trade_data_for_db['realized_pnl'] = realized_pnl.quantize(Decimal('0.000001'))
                # Store trade in DB
                await self.db.insert_trade(trade_data_for_db)
                # Update cumulative PnL calculator state
                await self.pnl_calculator.record_trade_pnl(trade_data_for_db, realized_pnl)
                logger.info(f"Trade Executed & Recorded: {symbol} {trade_side} {filled_qty:.6f} @ {avg_price:.4f}. R PnL: {realized_pnl:.4f}")
            else:
                logger.error(f"Failed to update position manager for trade {order_id} on {symbol}. Trade DB record might be missing or incomplete.")


        elif order_result and order_result.get('status') == 'REJECTED':
             logger.error(f"Order REJECTED for {symbol}. Reason: {order_result.get('rejectReason', 'Unknown')}. API Response: {order_result}")
             # Handle rejection (e.g., log, notify)
        elif order_result and order_result.get('status') == 'EXPIRED':
             logger.warning(f"Order EXPIRED for {symbol}. This shouldn't happen for MARKET orders typically. API Response: {order_result}")
             # Handle expiration
        elif order_result and order_result.get('status') == 'NEW':
             logger.warning(f"Order status NEW for {symbol} (e.g. LIMIT order). Monitoring required, not handled by executor directly. API Response: {order_result}")
             # Need separate logic to monitor open orders if using LIMITs etc.
        elif order_result and order_result.get('status') == 'FAILED': # Custom status from place_order on exception
             logger.error(f"Order placement failed for {symbol} before sending to API or during API call. Error: {order_result.get('error', 'Unknown')}")
        else:
             logger.error(f"Unhandled or failed order result for {symbol}: {order_result}")

```

---

**12. `core/trading_engine.py`**

```python
import asyncio
import time
from decimal import Decimal
from core.data_handler import DataHandler
from strategy.strategy_v2 import TradingStrategyV2
from execution.executor import Executor
from core.position_manager import PositionManager
from core.pnl_calculator import PnLCalculator
from database.db_manager import DatabaseManager
from interfaces.binance_interface import BinanceInterface # For type hinting
from utils.logger import setup_logger

logger = setup_logger(__name__)

class TradingEngine:
    def __init__(self, config, data_handler: DataHandler, strategy: TradingStrategyV2,
                 executor: Executor, position_manager: PositionManager,
                 pnl_calculator: PnLCalculator, db_manager: DatabaseManager,
                 binance_interface: BinanceInterface):
        self.config = config
        self.data_handler = data_handler
        self.strategy = strategy
        self.executor = executor
        self.position_manager = position_manager
        self.pnl_calculator = pnl_calculator
        self.db_manager = db_manager # For direct DB ops if needed
        self.interface = binance_interface # For actions like setting leverage
        self.is_running = False
        self.update_interval = config.get('update_interval_seconds', 60)
        self.trading_mode = config['trading_mode']
        self.leverage = int(config.get('leverage', 5))
        self.change_threshold_factor = 0.4 # From strategy description
        self.last_snapshot_time = 0
        self.snapshot_interval = 3600 # Update performance snapshot every hour


    async def initialize(self):
        """Initializes all components."""
        logger.info("--- Initializing Trading Engine ---")

        # Initialize DB first
        try:
             await self.db_manager.initialize_database()
        except Exception as e:
             logger.error(f"CRITICAL: Database initialization failed: {e}. Exiting.")
             return False

        # Initialize Binance client (needed for data fetching and position loading)
        if not await self.interface.initialize_client():
            logger.error("CRITICAL: Binance client initialization failed. Exiting.")
            return False

        # Load historical data
        if not await self.data_handler.initialize_data():
            logger.error("CRITICAL: Data handler initialization failed. Exiting.")
            await self.interface.close_client() # Clean up client
            return False

        # Load initial positions
        await self.position_manager.initialize_positions()

        # Load PnL state
        await self.pnl_calculator.initialize_pnl()

        # Set leverage for all symbols (if in live/testnet mode)
        if self.trading_mode in ['live', 'testnet']:
             logger.info(f"Setting leverage to {self.leverage}x for {len(self.data_handler.symbols)} symbols...")
             leverage_tasks = []
             for symbol in self.data_handler.symbols:
                 leverage_tasks.append(self.interface.change_leverage(symbol, self.leverage))
             results = await asyncio.gather(*leverage_tasks, return_exceptions=True)
             success_count = 0
             for res, symbol in zip(results, self.data_handler.symbols):
                  if isinstance(res, Exception):
                       logger.error(f"Failed to set leverage for {symbol}: {res}")
                  elif isinstance(res, dict) and res.get('status') != 'NOT_MODIFIED':
                      # Log success unless it was just not modified
                      logger.info(f"Leverage set/confirmed for {symbol}: {res}")
                      success_count +=1
                  elif isinstance(res, dict) and res.get('status') == 'NOT_MODIFIED':
                       success_count += 1 # Count as success if already set
             logger.info(f"Leverage setting process completed. Success/Confirmed: {success_count}/{len(self.data_handler.symbols)}")


        logger.info("--- Trading Engine Initialization Complete ---")
        return True

    async def run(self):
        """Main trading loop."""
        if not await self.initialize():
            return # Stop if initialization failed

        logger.info(">>> Starting Trading Engine Main Loop <<<")
        self.is_running = True

        # Start data streams
        await self.data_handler.start_websockets()
        # Start periodic funding rate updater task
        asyncio.create_task(self.data_handler.update_funding_rates_periodically())

        while self.is_running:
            start_time = time.time()
            try:
                # logger.debug("--- New Trading Cycle ---")

                # 1. Fetch latest data (already updated by websockets, just get references)
                # No explicit fetch needed here as websockets update data_handler

                # 2. Update Mark Prices & Get Current Equity Estimate
                # Fetch tickers for all active symbols to update mark prices
                symbols_to_update = self.data_handler.symbols
                if not symbols_to_update:
                     logger.warning("No symbols loaded in data handler, skipping cycle.")
                     await asyncio.sleep(self.update_interval)
                     continue

                ticker_tasks = {symbol: self.interface.get_ticker(symbol) for symbol in symbols_to_update}
                ticker_results = await asyncio.gather(*ticker_tasks.values(), return_exceptions=True)
                valid_tickers = {}
                for symbol, result in zip(ticker_tasks.keys(), ticker_results):
                    if isinstance(result, dict) and result.get('price') is not None:
                        valid_tickers[symbol] = result
                    elif isinstance(result, Exception):
                         logger.error(f"Failed to fetch ticker for {symbol}: {result}")
                    # else: logger.warning(f"No valid ticker data for {symbol}")

                # Update position manager with latest prices
                await self.position_manager.update_mark_prices(valid_tickers)

                # Get current equity estimate *after* updating PnL
                current_equity = await self.pnl_calculator.get_current_equity()


                # 3. Calculate Signals for all symbols
                signal_tasks = {}
                for symbol in self.data_handler.symbols:
                     df = await self.data_handler.get_latest_data(symbol)
                     if df is not None and not df.empty:
                         signal_tasks[symbol] = self.strategy.calculate_signals(symbol, df)
                     else:
                          logger.warning(f"Skipping signal calculation for {symbol}, no data available.")

                signal_results_list = await asyncio.gather(*signal_tasks.values(), return_exceptions=True)
                signal_results = dict(zip(signal_tasks.keys(), signal_results_list))


                # 4. Determine Target Positions and Execute Trades
                trade_tasks = []
                signals_to_store = []

                for symbol, result in signal_results.items():
                     if isinstance(result, dict) and result.get('final_factor') is not None:
                         final_factor_raw = result['final_factor'] # Factor before change threshold

                         # --- Apply Change Threshold Filter ---
                         last_factor = await self.data_handler.get_last_signal_factor(symbol)
                         # Calculate threshold based on recent factor volatility (use std dev of raw factor?)
                         # Need history of final_factor to calculate std dev, which is tricky state.
                         # Simplification: Use a fixed fraction of the factor itself or a predefined threshold?
                         # Using the description's approach: factor_std * 0.4
                         # Let's approximate factor_std with the target_signal_volatility for simplicity
                         # change_threshold_val = self.strategy.params.get('target_signal_volatility', 0.2) * self.change_threshold_factor
                         # Let's use a simpler relative change check for now
                         change_threshold_val = abs(last_factor * Decimal('0.1')) + Decimal('0.005') # e.g. 10% change + 0.005 fixed threshold


                         factor_change = Decimal(str(final_factor_raw)) - Decimal(str(last_factor))

                         final_factor_filtered = final_factor_raw # Default to new factor
                         if abs(factor_change) <= change_threshold_val:
                             # Change is too small, keep the previous factor
                             final_factor_filtered = last_factor
                             # logger.debug(f"{symbol}: Change threshold not met. Change={factor_change:.4f}, Thresh={change_threshold_val:.4f}. Keeping factor: {last_factor:.4f}")
                         else:
                              # Change is significant, update the last known factor state
                              await self.data_handler.set_last_signal_factor(symbol, final_factor_raw)
                              # logger.debug(f"{symbol}: Change threshold met. Change={factor_change:.4f}, Thresh={change_threshold_val:.4f}. Using new factor: {final_factor_raw:.4f}")


                         # Calculate target position
                         target_pos_usd = await self.executor.calculate_target_position_usd(symbol, float(final_factor_filtered), current_equity)

                         # Store signal data (including target) for analysis/DB
                         result['target_position_usd'] = float(target_pos_usd)
                         result['final_factor'] = float(final_factor_filtered) # Store the filtered factor
                         signals_to_store.append(result) # Add signal dict to list


                         # Get current position to pass to executor
                         current_pos = await self.position_manager.get_position(symbol)

                         # Create trade execution task
                         trade_tasks.append(self.executor.execute_trade(symbol, target_pos_usd, current_pos))

                     elif isinstance(result, Exception):
                          logger.error(f"Error calculating signal for {symbol}: {result}")
                     # else: Signal calculation returned None (e.g., insufficient data)

                # Execute all generated trade tasks concurrently
                if trade_tasks:
                     logger.debug(f"Executing {len(trade_tasks)} potential trade actions...")
                     await asyncio.gather(*trade_tasks)

                # Store signals to DB (optional)
                if signals_to_store:
                     store_signal_tasks = [self.db.insert_signal(sig) for sig in signals_to_store]
                     await asyncio.gather(*store_signal_tasks) # Store signals concurrently


                # 5. Periodic Performance Snapshot
                current_time = time.time()
                if current_time - self.last_snapshot_time >= self.snapshot_interval:
                     logger.info("Calculating and storing performance snapshot...")
                     await self.pnl_calculator.calculate_and_store_performance_snapshot(current_equity)
                     self.last_snapshot_time = current_time


            except asyncio.CancelledError:
                logger.info("Trading loop cancellation requested.")
                self.is_running = False
            except Exception as e:
                logger.exception("!!! CRITICAL ERROR in main trading loop !!!")
                # Decide whether to stop or try to continue
                # self.is_running = False # Option: Stop on critical error

            finally:
                 # Calculate loop duration and sleep accordingly
                 end_time = time.time()
                 loop_duration = end_time - start_time
                 sleep_time = max(0, self.update_interval - loop_duration)
                 # logger.debug(f"Cycle took {loop_duration:.2f}s. Sleeping for {sleep_time:.2f}s.")
                 if self.is_running: # Only sleep if we are supposed to continue
                     await asyncio.sleep(sleep_time)

        logger.info(">>> Trading Engine Main Loop Finished <<<")
        await self.cleanup()

    async def stop(self):
        """Stops the trading engine gracefully."""
        logger.info("--- Initiating Trading Engine Shutdown ---")
        self.is_running = False
        await self.data_handler.stop_websockets() # Stop data streams first
        # The loop will exit on the next iteration after is_running is False

    async def cleanup(self):
        """Cleans up resources like client connections and DB connections."""
        logger.info("Cleaning up resources...")
        await self.interface.close_client()
        await self.db_manager.close()
        logger.info("--- Trading Engine Shutdown Complete ---")

```

---

**13. `web_ui/api_server.py`**

```python
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from pathlib import Path
import uvicorn
import sys
from decimal import Decimal

# Add project root to sys.path to allow imports
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from database.db_manager import DatabaseManager # Assuming setup allows direct import
from core.position_manager import PositionManager # To get live positions
from core.pnl_calculator import PnLCalculator # To get live equity estimate
from utils.logger import setup_logger

logger = setup_logger("FastAPI_UI")

app = FastAPI(title="Crypto Trading Bot v2.0 Monitor")

# --- Globals (to be populated by main.py) ---
# These should be references to the actual running instances
db_manager: DatabaseManager = None
position_manager: PositionManager = None
pnl_calculator: PnLCalculator = None
config: dict = None
trading_engine_status: dict = {"running": False, "mode": "N/A"}

# Mount static files directory (HTML, JS, CSS)
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=static_dir), name="static")
else:
    logger.warning(f"Static directory not found at: {static_dir}. UI frontend may not load.")


# --- Helper Function ---
def decimal_default(obj):
    """Convert Decimals to strings for JSON serialization."""
    if isinstance(obj, Decimal):
        # Format to a reasonable number of decimal places for display
        # Adjust based on typical price/quantity precision needed
        return "{:.8f}".format(obj) # Format as string with 8 decimal places
    raise TypeError

# --- API Endpoints ---

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """Serves the main HTML page."""
    index_path = static_dir / "index.html"
    if not index_path.exists():
        raise HTTPException(status_code=404, detail="index.html not found")
    with open(index_path, "r") as f:
        html_content = f.read()
    return HTMLResponse(content=html_content)

@app.get("/api/status")
async def get_status():
    """Returns the current status of the trading bot."""
    if not pnl_calculator or not position_manager:
         return {"error": "Backend components not fully initialized."}

    # Fetch live data
    try:
        current_equity = await pnl_calculator.get_current_equity()
        # Get some basic position stats
        active_positions = await position_manager.get_all_positions()
        num_positions = len(active_positions)
        total_value = await position_manager.get_total_positions_value()
        # Get last performance record for cumulative PnL
        last_perf = await db_manager.get_latest_performance(trading_engine_status['mode']) if db_manager else None

        return JSONResponse(content={
            "appName": config.get("app_name", "N/A") if config else "N/A",
            "tradingMode": trading_engine_status.get('mode', 'N/A'),
            "isRunning": trading_engine_status.get('running', False),
            "currentEquity": current_equity,
            "activePositionsCount": num_positions,
            "totalPositionValueUSD": total_value,
            "cumulativeRealizedPnL": last_perf.get('realized_pnl_cumulative', 0) if last_perf else 0,
            "cumulativeFees": last_perf.get('total_fees_cumulative', 0) if last_perf else 0,
            "cumulativeTrades": last_perf.get('trade_count_cumulative', 0) if last_perf else 0,
            "timestamp": datetime.now().isoformat()
        }, default=decimal_default) # Use custom serializer for Decimals
    except Exception as e:
         logger.error(f"Error fetching status: {e}")
         raise HTTPException(status_code=500, detail=f"Internal server error fetching status: {e}")


@app.get("/api/config")
async def get_config():
    """Returns non-sensitive parts of the configuration."""
    if not config:
         raise HTTPException(status_code=500, detail="Configuration not loaded")
    # Return only safe parameters
    safe_config = {
        "app_name": config.get("app_name"),
        "trading_mode": config.get("trading_mode"),
        "log_level": config.get("log_level"),
        "update_interval_seconds": config.get("update_interval_seconds"),
        "kline_interval": config.get("kline_interval"),
        "universe": config.get("universe"),
        "leverage": config.get("leverage"),
        "initial_capital_usd": config.get("initial_capital_usd"),
        "target_signal_volatility": config.get("target_signal_volatility"),
        "web_ui": config.get("web_ui")
        # DO NOT RETURN API KEYS or DB PASSWORD
    }
    return JSONResponse(content=safe_config)

@app.get("/api/positions")
async def get_positions():
    """Returns the current open positions."""
    if not position_manager:
        raise HTTPException(status_code=500, detail="Position Manager not available")
    try:
        positions = await position_manager.get_all_positions()
        # Convert Decimal values to strings for JSON
        return JSONResponse(content=list(positions.values()), default=decimal_default)
    except Exception as e:
        logger.error(f"Error fetching positions: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch positions: {e}")


@app.get("/api/trades")
async def get_trades(symbol: str = None, limit: int = 100):
    """Returns recent trade history, optionally filtered by symbol."""
    if not db_manager:
        raise HTTPException(status_code=500, detail="Database Manager not available")
    try:
        mode = trading_engine_status.get('mode', config.get('trading_mode')) # Get current mode
        trades = await db_manager.get_trades(mode=mode, symbol=symbol, limit=limit)
         # Convert datetime and Decimal
        for trade in trades:
             if isinstance(trade.get('timestamp'), datetime):
                 trade['timestamp'] = trade['timestamp'].isoformat()
             if isinstance(trade.get('trade_time'), datetime):
                  trade['trade_time'] = trade['trade_time'].isoformat()
        return JSONResponse(content=trades, default=decimal_default)
    except Exception as e:
        logger.error(f"Error fetching trades: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch trades: {e}")


@app.get("/api/performance/history")
async def get_performance_history(limit: int = 200): # Limit history points for chart
    """Returns historical performance data for charting."""
    if not db_manager:
        raise HTTPException(status_code=500, detail="Database Manager not available")
    try:
        mode = trading_engine_status.get('mode', config.get('trading_mode'))
        history = await db_manager.get_performance_history(mode=mode, limit=limit)
         # Convert datetime and Decimal
        for record in history:
             if isinstance(record.get('timestamp'), datetime):
                 record['timestamp'] = record['timestamp'].isoformat()
        return JSONResponse(content=history, default=decimal_default)
    except Exception as e:
        logger.error(f"Error fetching performance history: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch performance history: {e}")


@app.get("/api/symbols")
async def get_symbols():
     """Returns the list of symbols currently being traded or monitored."""
     # Assuming data_handler holds the active symbols list
     if 'data_handler' in globals() and globals()['data_handler'] is not None:
          return JSONResponse(content=globals()['data_handler'].symbols)
     elif config:
          # Fallback to config if data_handler not available (e.g. during init)
          return JSONResponse(content=config.get("universe",{}).get("symbol_list", []))
     else:
          return JSONResponse(content=[])


# --- Function to run the server (called from main.py) ---
def start_server(app_config, db_instance, pos_manager_instance, pnl_calc_instance, engine_status_ref, main_config):
    """Starts the Uvicorn server."""
    global db_manager, position_manager, pnl_calculator, trading_engine_status, config
    db_manager = db_instance
    position_manager = pos_manager_instance
    pnl_calculator = pnl_calc_instance
    trading_engine_status = engine_status_ref # Share the mutable status dict
    config = main_config # Share main config

    ui_config = app_config.get('web_ui', {})
    host = ui_config.get('host', '127.0.0.1')
    port = ui_config.get('port', 8000)

    logger.info(f"Starting FastAPI UI server on http://{host}:{port}")
    # Note: Running uvicorn.run directly blocks. It should be run in a separate process
    # or managed by the main async loop using Server.serve()
    # uvicorn.run(app, host=host, port=port) # This will block if called directly

    # Instead, return the app instance to be run by main.py's uvicorn call
    return app

```

---

**14. `web_ui/static/index.html`** (Basic Example)

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Crypto Bot Monitor</title>
    <link rel="stylesheet" href="/static/styles.css">
    <!-- Include Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <!-- Include moment.js for time formatting with Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/moment@2.29.1/moment.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js"></script>

</head>
<body>
    <h1>Multi-Factor Crypto Bot v2.0 Monitor</h1>

    <div class="container">
        <!-- Status Section -->
        <div class="card">
            <h2>Status</h2>
            <p><strong>App Name:</strong> <span id="status-app-name">Loading...</span></p>
            <p><strong>Mode:</strong> <span id="status-mode">Loading...</span></p>
            <p><strong>Running:</strong> <span id="status-running">Loading...</span></p>
            <p><strong>Equity (USDT):</strong> <span id="status-equity">Loading...</span></p>
            <p><strong>Active Positions:</strong> <span id="status-pos-count">Loading...</span></p>
            <p><strong>Total Pos Value (USD):</strong> <span id="status-pos-value">Loading...</span></p>
            <p><strong>Cumulative Realized PnL:</strong> <span id="status-cum-rpnl">Loading...</span></p>
            <p><strong>Cumulative Fees:</strong> <span id="status-cum-fees">Loading...</span></p>
             <p><strong>Cumulative Trades:</strong> <span id="status-cum-trades">Loading...</span></p>
            <p><em>Last Updated: <span id="status-timestamp">Never</span></em></p>
        </div>

        <!-- Performance Chart Section -->
        <div class="card">
            <h2>Performance History (Equity)</h2>
            <canvas id="performanceChart"></canvas>
        </div>

        <!-- Positions Section -->
        <div class="card">
            <h2>Current Positions</h2>
            <table id="positions-table">
                <thead>
                    <tr>
                        <th>Symbol</th>
                        <th>Quantity</th>
                        <th>Entry Price</th>
                        <th>Mark Price</th>
                        <th>Unrealized PnL</th>
                        <th>Leverage</th>
                        <th>Margin (Est.)</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Position rows will be added here -->
                </tbody>
            </table>
        </div>

        <!-- Trades Section -->
        <div class="card">
            <h2>Recent Trades</h2>
             <label for="trade-symbol-filter">Filter by Symbol:</label>
             <select id="trade-symbol-filter">
                 <option value="">All Symbols</option>
                 <!-- Options added dynamically -->
             </select>
             <button onclick="fetchTrades()">Refresh Trades</button>
             <br><br>
            <table id="trades-table">
                <thead>
                    <tr>
                        <th>Timestamp</th>
                        <th>Symbol</th>
                        <th>Side</th>
                        <th>Qty</th>
                        <th>Price</th>
                        <th>Value</th>
                        <th>Fee</th>
                        <th>Realized PnL</th>
                        <th>Order ID</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Trade rows will be added here -->
                </tbody>
            </table>
        </div>

         <!-- Config Section -->
        <div class="card">
            <h2>Configuration</h2>
            <pre id="config-details">Loading...</pre>
        </div>
    </div>

    <script src="/static/script.js"></script>
</body>
</html>
```

---

**15. `web_ui/static/script.js`** (Basic Example)

```javascript
let performanceChart = null;
const API_BASE_URL = '/api'; // Assuming FastAPI runs on the same host/port

// --- Helper Functions ---
function formatNumber(numStr, decimals = 2) {
    if (numStr === null || numStr === undefined) return 'N/A';
    const num = parseFloat(numStr);
    if (isNaN(num)) return 'N/A';
    return num.toFixed(decimals);
}

function formatTimestamp(isoString) {
    if (!isoString) return 'N/A';
    try {
        return new Date(isoString).toLocaleString();
    } catch (e) {
        return isoString; // Return original if parsing fails
    }
}

function updateElementText(id, text, formatter = null) {
    const element = document.getElementById(id);
    if (element) {
        element.textContent = formatter ? formatter(text) : (text !== null && text !== undefined ? text : 'N/A');
    } else {
        console.warn(`Element with id ${id} not found.`);
    }
}

// --- Data Fetching Functions ---
async function fetchData(endpoint) {
    try {
        const response = await fetch(`${API_BASE_URL}${endpoint}`);
        if (!response.ok) {
            console.error(`Error fetching ${endpoint}: ${response.status} ${response.statusText}`);
            const errorData = await response.json().catch(() => ({ detail: "Failed to parse error" }));
            console.error("Error details:", errorData);
            return null;
        }
        return await response.json();
    } catch (error) {
        console.error(`Network error fetching ${endpoint}:`, error);
        return null;
    }
}

async function fetchStatus() {
    const data = await fetchData('/status');
    if (data) {
        updateElementText('status-app-name', data.appName);
        updateElementText('status-mode', data.tradingMode);
        updateElementText('status-running', data.isRunning ? 'Running' : 'Stopped', (val) => val); // Keep boolean text
        updateElementText('status-equity', data.currentEquity, (val) => formatNumber(val, 2));
        updateElementText('status-pos-count', data.activePositionsCount, (val) => val);
        updateElementText('status-pos-value', data.totalPositionValueUSD, (val) => formatNumber(val, 2));
        updateElementText('status-cum-rpnl', data.cumulativeRealizedPnL, (val) => formatNumber(val, 4));
        updateElementText('status-cum-fees', data.cumulativeFees, (val) => formatNumber(val, 6));
        updateElementText('status-cum-trades', data.cumulativeTrades, (val) => val);
        updateElementText('status-timestamp', data.timestamp, formatTimestamp);
    }
}

async function fetchPositions() {
    const positions = await fetchData('/positions');
    const tableBody = document.getElementById('positions-table')?.querySelector('tbody');
    if (positions && tableBody) {
        tableBody.innerHTML = ''; // Clear existing rows
        if (positions.length === 0) {
             tableBody.innerHTML = '<tr><td colspan="7">No active positions.</td></tr>';
             return;
        }
        positions.forEach(pos => {
            const row = tableBody.insertRow();
            row.innerHTML = `
                <td>${pos.symbol || 'N/A'}</td>
                <td class="${parseFloat(pos.quantity || 0) >= 0 ? 'text-green' : 'text-red'}">${formatNumber(pos.quantity, 6)}</td>
                <td>${formatNumber(pos.entry_price, 4)}</td>
                <td>${formatNumber(pos.mark_price, 4)}</td>
                <td class="${parseFloat(pos.unrealized_pnl || 0) >= 0 ? 'text-green' : 'text-red'}">${formatNumber(pos.unrealized_pnl, 4)}</td>
                <td>${pos.leverage || 'N/A'}x</td>
                <td>${formatNumber(pos.initial_margin, 2)}</td>
            `;
        });
    } else if (tableBody) {
        tableBody.innerHTML = '<tr><td colspan="7">Failed to load positions.</td></tr>';
    }
}

async function fetchTrades() {
     const symbolFilter = document.getElementById('trade-symbol-filter').value;
     const limit = 100; // Or get from UI element
     let url = `/trades?limit=${limit}`;
     if (symbolFilter) {
         url += `&symbol=${symbolFilter}`;
     }

    const trades = await fetchData(url);
    const tableBody = document.getElementById('trades-table')?.querySelector('tbody');
    if (trades && tableBody) {
        tableBody.innerHTML = ''; // Clear existing rows
         if (trades.length === 0) {
             tableBody.innerHTML = `<tr><td colspan="9">No recent trades found${symbolFilter ? ` for ${symbolFilter}` : ''}.</td></tr>`;
             return;
         }
        trades.forEach(trade => {
            const row = tableBody.insertRow();
            const quoteQty = parseFloat(trade.quote_quantity || 0);
            const realizedPnl = parseFloat(trade.realized_pnl || 0);
             const sideClass = (trade.side === 'BUY') ? 'text-green' : 'text-red';
             const pnlClass = realizedPnl >= 0 ? 'text-green' : 'text-red';
            row.innerHTML = `
                <td>${formatTimestamp(trade.timestamp)}</td>
                <td>${trade.symbol || 'N/A'}</td>
                <td class="${sideClass}">${trade.side || 'N/A'}</td>
                <td>${formatNumber(trade.quantity, 6)}</td>
                <td>${formatNumber(trade.price, 4)}</td>
                <td>${formatNumber(quoteQty.toString(), 2)}</td>
                <td>${formatNumber(trade.commission, 6)}</td>
                <td class="${pnlClass}">${formatNumber(realizedPnl.toString(), 4)}</td>
                <td>${trade.order_id || 'N/A'}</td>
            `;
        });
    } else if (tableBody) {
        tableBody.innerHTML = '<tr><td colspan="9">Failed to load trades.</td></tr>';
    }
}


async function fetchPerformanceHistory() {
    const history = await fetchData('/performance/history?limit=500'); // Fetch more points for chart
    if (history && history.length > 0) {
        const ctx = document.getElementById('performanceChart').getContext('2d');

        const labels = history.map(p => p.timestamp);
        const equityData = history.map(p => parseFloat(p.total_equity));

        if (performanceChart) {
            performanceChart.data.labels = labels;
            performanceChart.data.datasets[0].data = equityData;
            performanceChart.update();
        } else {
            performanceChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Total Equity (USDT)',
                        data: equityData,
                        borderColor: 'rgb(75, 192, 192)',
                        tension: 0.1,
                        pointRadius: 1, // Smaller points for cleaner look
                        borderWidth: 1.5
                    }]
                },
                options: {
                    scales: {
                        x: {
                            type: 'time',
                            time: {
                                unit: 'hour', // Adjust based on data frequency
                                tooltipFormat: 'YYYY-MM-DD HH:mm',
                                displayFormats: {
                                     hour: 'MMM D HH:mm',
                                     day: 'MMM D'
                                 }
                            },
                            title: {
                                display: true,
                                text: 'Time'
                            },
                            ticks: {
                                 maxRotation: 45,
                                 minRotation: 45,
                                 autoSkip: true,
                                 maxTicksLimit: 20 // Limit number of x-axis labels
                             }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Equity (USDT)'
                            },
                             ticks: {
                                 callback: function(value, index, values) {
                                     // Optional: Format large numbers (e.g., 10k)
                                     return value.toLocaleString();
                                 }
                             }
                        }
                    },
                     plugins: {
                         tooltip: {
                             mode: 'index',
                             intersect: false,
                         },
                         legend: {
                              display: false // Hide legend if only one dataset
                         }
                     },
                     responsive: true,
                     maintainAspectRatio: false // Allow chart to resize height
                }
            });
        }
    } else {
         console.log("No performance history data to plot.");
         // Optional: Display a message on the canvas
         const canvas = document.getElementById('performanceChart');
         const ctx = canvas?.getContext('2d');
         if (ctx) {
             ctx.clearRect(0, 0, canvas.width, canvas.height);
             ctx.textAlign = 'center';
             ctx.fillText('No performance data available.', canvas.width / 2, canvas.height / 2);
         }
    }
}

async function fetchConfig() {
     const data = await fetchData('/config');
     const preElement = document.getElementById('config-details');
     if (data && preElement) {
         preElement.textContent = JSON.stringify(data, null, 2); // Pretty print JSON
     } else if (preElement) {
          preElement.textContent = "Failed to load configuration.";
     }
}

async function populateSymbolFilter() {
     const symbols = await fetchData('/symbols');
     const selectElement = document.getElementById('trade-symbol-filter');
     if (symbols && selectElement) {
         // Clear existing options except the "All Symbols" default
         selectElement.length = 1;
         symbols.forEach(symbol => {
             const option = document.createElement('option');
             option.value = symbol;
             option.text = symbol;
             selectElement.appendChild(option);
         });
     }
}


// --- Initial Load and Interval Updates ---
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM Loaded. Starting initial data fetch.");
    fetchStatus();
    fetchPositions();
    populateSymbolFilter().then(fetchTrades); // Fetch trades after symbols are loaded
    fetchPerformanceHistory();
    fetchConfig();

    // Set intervals to refresh data
    setInterval(fetchStatus, 10000); // Refresh status every 10 seconds
    setInterval(fetchPositions, 15000); // Refresh positions every 15 seconds
    setInterval(fetchPerformanceHistory, 60000); // Refresh chart every 60 seconds
    // Trades are refreshed manually by default button click
});

```

---

**16. `web_ui/static/styles.css`** (Basic Example)

```css
body {
    font-family: sans-serif;
    background-color: #f4f4f4;
    margin: 0;
    padding: 20px;
    color: #333;
}

h1 {
    text-align: center;
    color: #2c3e50;
    margin-bottom: 30px;
}

.container {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); /* Responsive grid */
    gap: 20px;
}

.card {
    background-color: #fff;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
}

.card h2 {
    margin-top: 0;
    color: #3498db;
    border-bottom: 1px solid #eee;
    padding-bottom: 10px;
    margin-bottom: 15px;
}

table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 15px;
}

th, td {
    border: 1px solid #ddd;
    padding: 8px 10px;
    text-align: left;
    font-size: 0.9em;
}

th {
    background-color: #f2f2f2;
    font-weight: bold;
}

tbody tr:nth-child(even) {
    background-color: #f9f9f9;
}

#positions-table td:nth-child(2), /* Quantity */
#positions-table td:nth-child(5), /* PnL */
#trades-table td:nth-child(3),   /* Side */
#trades-table td:nth-child(8)    /* Realized PnL */
{
    font-weight: bold;
}

.text-green {
    color: #2ecc71; /* Green */
}

.text-red {
    color: #e74c3c; /* Red */
}

#performanceChart {
    max-height: 350px; /* Limit chart height */
    width: 100%;
}

pre {
    background-color: #eee;
    padding: 15px;
    border-radius: 4px;
    font-size: 0.85em;
    overflow-x: auto; /* Allow horizontal scrolling for long lines */
    white-space: pre-wrap; /* Wrap long lines */
    word-wrap: break-word;
}

button {
     padding: 8px 15px;
     background-color: #3498db;
     color: white;
     border: none;
     border-radius: 4px;
     cursor: pointer;
     font-size: 0.9em;
     margin-left: 10px;
 }

 button:hover {
     background-color: #2980b9;
 }

 select, label {
     font-size: 0.9em;
     padding: 5px;
 }
```

---

**17. `requirements.txt`**

```
pandas
numpy
python-binance>=1.0.16 # Check for latest async compatible version
websockets
mariadb
PyYAML
fastapi
uvicorn[standard]
requests # Potentially useful for other tasks
asyncio
# plotly (optional, if doing server-side charts)
# aiohttp (optional, alternative async http client)
# schedule (optional, for specific job scheduling)
```

---

**18. `main.py`** (Entry Point)

```python
import asyncio
import signal
import sys
import yaml
from pathlib import Path
import uvicorn # Import uvicorn to run FastAPI app

# Add project root to sys.path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from utils.logger import setup_logger
from database.db_manager import DatabaseManager
from interfaces.binance_interface import BinanceInterface
from strategy.strategy_v2 import TradingStrategyV2
from core.data_handler import DataHandler
from core.position_manager import PositionManager
from core.pnl_calculator import PnLCalculator
from execution.executor import Executor
from core.trading_engine import TradingEngine
from web_ui.api_server import start_server # Import the function that returns the app

# --- Global Variables ---
config = None
logger = None
trading_engine_instance: TradingEngine = None
shutdown_event = asyncio.Event()
# Shared status dictionary for UI
engine_status = {"running": False, "mode": "N/A"}

# --- Signal Handling ---
def handle_shutdown_signal(signum, frame):
    logger.info(f"Received shutdown signal {signum}. Initiating graceful shutdown...")
    # Use call_soon_threadsafe if called from a different thread context (signals often are)
    asyncio.get_event_loop().call_soon_threadsafe(shutdown_event.set)

async def main():
    global config, logger, trading_engine_instance, engine_status

    # --- Load Configuration ---
    config_path = project_root / 'config' / 'config.yaml'
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"ERROR: Configuration file not found at {config_path}")
        sys.exit(1)
    except yaml.YAMLError as e:
        print(f"ERROR: Error parsing configuration file: {e}")
        sys.exit(1)

    # --- Setup Logger ---
    log_level_str = config.get('log_level', 'INFO').upper()
    log_level = getattr(logging, log_level_str, logging.INFO)
    logger = setup_logger(config.get('app_name', 'CryptoBot'), level=log_level)
    logger.info("--- Starting Multi-Factor Crypto Trading Bot v2.0 ---")
    logger.info(f"Loaded configuration from {config_path}")
    engine_status["mode"] = config.get('trading_mode', 'N/A')


    # --- Initialize Components ---
    try:
        db_manager = DatabaseManager(config)
        binance_interface = BinanceInterface(config)
        position_manager = PositionManager(config, db_manager, binance_interface)
        pnl_calculator = PnLCalculator(config, db_manager, position_manager)
        # Pass pnl_calculator and data_handler to executor
        data_handler = DataHandler(config, binance_interface, db_manager)
        executor = Executor(config, binance_interface, db_manager, position_manager, pnl_calculator, data_handler)
        strategy = TradingStrategyV2(config)
        trading_engine_instance = TradingEngine(
            config, data_handler, strategy, executor, position_manager,
            pnl_calculator, db_manager, binance_interface
        )
    except Exception as e:
        logger.exception("CRITICAL: Failed to initialize core components.")
        sys.exit(1)

    # --- Start FastAPI Server ---
    # Get the FastAPI app instance
    fastapi_app = start_server(config, db_manager, position_manager, pnl_calculator, engine_status, config)

    # Configure Uvicorn server (run within the async context)
    uvicorn_config = uvicorn.Config(
        app=fastapi_app,
        host=config.get('web_ui', {}).get('host', '127.0.0.1'),
        port=config.get('web_ui', {}).get('port', 8000),
        log_level=config.get('log_level', 'info').lower(), # Match uvicorn levels
        loop="asyncio" # Ensure it uses the current asyncio loop
    )
    server = uvicorn.Server(uvicorn_config)

    # --- Start Trading Engine and Server ---
    logger.info("Starting Trading Engine task...")
    engine_task = asyncio.create_task(trading_engine_instance.run(), name="TradingEngine")
    engine_status["running"] = True # Update status after task creation

    logger.info("Starting FastAPI Uvicorn server task...")
    # Run server.serve in a task so it doesn't block main
    server_task = asyncio.create_task(server.serve(), name="FastAPI_Server")

    # --- Wait for Shutdown Signal ---
    logger.info("Bot is running. Press Ctrl+C to shut down gracefully.")
    await shutdown_event.wait() # Wait until the signal handler sets this event

    # --- Initiate Shutdown ---
    logger.info("Shutdown process initiated...")
    engine_status["running"] = False # Update status

    # 1. Stop the trading engine first (allows final cleanup/logging)
    if trading_engine_instance:
        await trading_engine_instance.stop()
        # Wait for engine task to finish its cleanup
        try:
            await asyncio.wait_for(engine_task, timeout=30.0) # Give 30s for cleanup
        except asyncio.TimeoutError:
            logger.warning("Trading engine task did not finish cleanup within timeout.")
            engine_task.cancel() # Force cancel if stuck
        except Exception as e:
             logger.error(f"Error during trading engine task completion: {e}")


    # 2. Stop the Uvicorn server
    logger.info("Stopping FastAPI server...")
    # Uvicorn 0.15+ has should_exit and shutdown()
    if hasattr(server, 'should_exit'):
        server.should_exit = True
    # Give server task time to shut down gracefully
    if not server_task.done():
        try:
            # Wait briefly for connections to close - direct shutdown might be better
            # await asyncio.sleep(2) # Short delay
            # server_task.cancel() # Cancel if still running
            await asyncio.wait_for(server_task, timeout=10.0)
        except asyncio.TimeoutError:
            logger.warning("FastAPI server task did not shut down within timeout.")
            if not server_task.done(): server_task.cancel() # Force cancel
        except asyncio.CancelledError:
             logger.info("FastAPI server task cancelled.") # Expected if cancelled
        except Exception as e:
             logger.error(f"Error stopping FastAPI server task: {e}")


    logger.info("All tasks stopped. Exiting application.")


if __name__ == "__main__":
    import logging # Need logging here for logger setup

    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, handle_shutdown_signal)  # Handle Ctrl+C
    signal.signal(signal.SIGTERM, handle_shutdown_signal) # Handle kill command

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
         # This might catch the initial Ctrl+C before the handler fully runs
         print("\nKeyboardInterrupt caught in __main__. Exiting.")
    except Exception as e:
         # Log any top-level exceptions during startup/shutdown not caught elsewhere
         print(f"\nFATAL ERROR in main execution: {e}")
         if logger: # Log if logger was initialized
             logger.exception("Fatal error during main execution.")
```

---

**다음 단계:**

1.  **라이브러리 설치:** `pip install -r requirements.txt`
2.  **MariaDB 설정:** MariaDB 서버를 설치하고, `config.yaml`에 지정된 사용자(`crypto_bot_user`)와 데이터베이스(`crypto_trading_v2`)를 생성하고 권한을 부여하세요.
3.  **설정 파일 수정:** `config/config.yaml` 파일을 열고 데이터베이스 비밀번호, 거래 모드, (live/testnet 사용 시) API 키 등을 설정하세요. **API 키는 절대로 코드나 설정 파일에 직접 넣지 말고 환경 변수나 보안 저장소를 사용하는 것이 좋습니다.**
4.  **실행:** 터미널에서 프로젝트 루트 디렉토리로 이동한 후 `python main.py` 명령어로 봇을 실행합니다.
5.  **웹 UI 접속:** 브라우저에서 `http://127.0.0.1:8000` (또는 `config.yaml`에 설정한 주소)으로 접속하여 모니터링합니다.
6.  **테스트 및 디버깅:** 페이퍼 트레이딩 모드로 시작하여 로그를 주의 깊게 관찰하고, 전략 로직, 주문 실행, PnL 계산 등이 예상대로 작동하는지 확인합니다. 웹 UI를 통해 상태를 모니터링합니다.
7.  **개선:** 필요에 따라 오류 처리 강화, 리스크 관리 로직 구체화, UI 개선 등을 진행합니다.

이 코드는 방대하며 여러 비동기 컴포넌트가 상호작용하므로, 실제 운영 전 철저한 테스트가 필수적입니다.